---
title: 혼자 공부하는 머신러닝 딥러닝
Author:
  - 박해선 저 | 한빛미디어
Status: Currently Reading
Topics: ML and DL
tags:
  - Book
Kanban-Cover: "![Book Cover|300](https://image.yes24.com/goods/96024871/XL)"
draft: false
---
# Ch.2 데이터 다루기 (classification)
## Sampling Bias
훈련 세트와 테스트 세트에 샘플이 골고루 섞여 있지 않으면 샘플링이 한쪽으로 치우칠 수 있음.
![|750](https://i.imgur.com/PJiMOIe.png)

### Problem

### Solution: Numpy
dataset: fish-market 

```python
import glob
import numpy as np
import pandas as pd

dir = glob.glob("data/fish-market/*.csv")
fish = pd.read_csv(dir[0])

bream = fish.loc[fish['Species'] == 'Bream']
smelt = fish.loc[fish['Species'] == 'Smelt']
```

![|625](https://i.imgur.com/hzzZlh0.png)

#### prepare dataset
```python
# len(bream) = 35
bream_length = bream['Length2']
bream_weight = bream['Weight']

# len(smelt) = 14
smelt_length = smelt['Length2']
smelt_weight = smelt['Weight']

length = pd.concat([bream_length, smelt_length])
weight = pd.concat([bream_weight, smelt_weight])

index = np.arange(49)
fish_input = np.column_stack([length, weight])
fish_target = np.concatenate([np.ones(35), np.zeros(14)])

>> array([[ 25.4, 242. ], [ 26.3, 290. ], [ 26.5, 340. ], [ 29. , 363. ], [ 29. , 430. ], [ 29.7, 450. ], [ 29.7, 500. ], ...
>> array([1., 1., 1., 1., ...
```

#### randomize dataset
```python
np.random.seed(42)
index = np.arange(49)
np.random.shuffle(index)
>> array([13, 45, 47, 44, 17, 27, 26, ...
```

#### np’s array indexing
```python
train_input = fish_input[index[:35]]
train_target = fish_target[index[:35]]

# 셔플된 인덱스의 값이 13이므로 인풋의 13번째 인덱스 값이 훈련 세트의 첫번째 값과 동일할것이다.
print(fish_input[13], train_input[0])
>> [ 32. 340.] [ 32. 340.]

test_input = fish_input[index[35:]]
test_target = fish_target[index[35:]]
```

#### plot
##### before
```python
import matplotlib.pylab as plt

plt.scatter(bream_length, bream_weight)
plt.scatter(smelt_length, smelt_weight)
plt.xlabel('length')
plt.xlabel('weight')
plt.show()
```
![|550](https://i.imgur.com/80zK3ie.png)

##### after
- blue: train
- orange: test
```python
import matplotlib.pylab as plt

plt.scatter(train_input[:,0], train_input[:,1])
plt.scatter(test_input[:,0], test_input[:,1])
plt.xlabel('length')
plt.xlabel('weight')
plt.show()
```

![|575](https://i.imgur.com/IOCfZbn.png)

#### Train and Predict
```python
from sklearn.neighbors import KNeighborsClassifier
kn = KNeighborsClassifier()

kn.fit(train_input, train_target)
kn.score(test_input, test_target)
kn.predict(test_input)
>> array([0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0.])
```

```python
test_target
>> array([0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0.])
```

```python
if (kn.predict(test_input) == test_target).all:
    print("Successfully Predicted.")
else:
    print("Prediction Failed")
>> Successfully Predicted.
```


## Standard Score (전처리) - The ‘z’ Score
### Problem
![|550](https://i.imgur.com/wypO5qJ.png)

→ X 축은 범위가 좁고(10 ~ 40), Y축은 범위가 넓다 (0 ~ 1000). 따라서 y축으로 조금만 멀어져도 거리가 아주 큰 값으로 계산된다. 이때문에 오른쪽위 샘플이 이웃으로 선택되지 못했다.

### Solution: Z Score
표준점수는 각 특성값이 평균에서 표준편차의 몇 배만큼 떨어져 있는지를 나타낸다. 
**이를 통해 실제 특성값의 크기와 상관없이 동일한 조건으로 비교가 가능하다.**

> [!note] Standard Score & Standard Deviation
> 분산(distribution)은 데이터에서 평균을 뺀 값을 모두 제곱한 당므 평균을 내어 구합니다. 표준 편차는 분산의 제곱근으로 데이터가 분산된 정도를 나타냅니다. 표준점수는 각 데이터가 원점에서 몇 표준편차만큼 떨어져 있는지를 나타내는 값입니다.

#### Scale
계산하는 방법은 간단합니다. 평균을 빼고 표준편차를 나누어준다.

```python
mean = np.mean(train_input, axis=0)
std = np.std(train_input, axis=0)
print(mean,std)
>> [ 28.29428571 483.35714286] [ 9.54606704 323.47456715]
```

```python
train_scaled = (train_input - mean) / std
```

![|675](https://i.imgur.com/KMZAxOJ.png)

```python
new = ([25, 150] - mean) /std
plt.scatter(train_scaled[:,0], train_scaled[:,1])
plt.scatter(new[0], new[1], marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

![|550](https://i.imgur.com/r3E3KUl.png)

#### Train
Before
```python
kn.fit(train_input, train_target)
kn.score(train_input, train_target)
kn.predict([[25,150]])

# 지표상으로는 1이 나와야하는데 0이 나옴
>> array([0.])
```

After
```python
kn_2 = KNeighborsClassifier()
kn_2.fit(train_scaled, train_target)
kn_2.score(train_scaled, train_target)
kn_2.predict([new])

# 도미로 예측 성공
>> array([1.])
```

### Conclusion
대부분의 머신러닝 알고리즘은 특성의 스케일이 다르면 잘 작동하지 않는다. 이를위해 특성을 표준 점수로 변환했다. 
스케일을 조정하는 방법은 표준점수 말고도 더 많지만 대부분의 경우 표준점수로 충분하다.

---

# Ch.3 Regression (prediction)
> [!note] 회귀란?
19세기 통계학자이자 사회학자인 프랜시스 골턴이 처음 사용했고, 키가 큰 사람의 아이가 부모보다 크지 않다는 사실을 관찰했습니다. 
이를 현상을 ‘평균으로 회귀한다’ 라고 표현하고 그 후 **두 변수 사이의 상관관계를 분석하는 방법**을 회귀라고 부르게되었다.

## Regression and Coefficient Determination (R^2)
### Dataset
```python
perch = fish.loc[fish['Species'] == 'Perch']
perch_length = perch['Length2']
perch_weight = perch['Weight']

plt.scatter(perch_length, perch_weight)
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```
![|700](https://i.imgur.com/u4wd8a1.png)

#### split train and test
```python
from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(
    perch_length, perch_weight, random_state=42)
```

이번 예제에서는 특성을 1개만 사용하므로 수동으로 2차원 배열을 만들어야한다. numpy의 reshape을 사용.
```python
test_array = np.array([1,2,3,4])
print(test_array.shape)
>> (4,)

test_array = test_array.reshape(2,2)
print(test_array.shape)
>> (2, 2)
```

### Train
```python
from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(
    perch_length, perch_weight, random_state=42)

train_input, test_input, train_target, test_target = \
np.array(train_input), np.array(test_input), np.array(train_target), np.array(test_target)
```

```python
from sklearn.neighbors import KNeighborsRegressor

train_input = train_input.reshape(-1, 1)
train_target = train_target.reshape(-1, 1)

knr = KNeighborsRegressor()
knr.fit(train_input, train_target)
knr.score(train_input, train_target)
>> 0.9698823289099254
```

### Evaluate: 결정 계수 구하기
![|775](https://i.imgur.com/1JXcVCR.png)

mae는 타깃과 예측의 절댓값 오차를 평균하여 반환한다.

```python
from sklearn.metrics import mean_absolute_error

test_input = test_input.reshape(-1, 1)
test_target = test_target.reshape(-1, 1)

test_prediction = knr.predict(test_input)
mae = mean_absolute_error(test_target, test_prediction)
mae
>> 19.157142857142862
```

결과와 예측이 평균적으로 19g 정도 타깃과 다르다는것을 알 수 있다. 

지금까지는:
train 세트를 사용해 모델을 훈련하고
test 세트를 사용해 모델을 평가했습니다.

그럼 train 세트를 사용해 평가를 해보면 어떨까? 즉, score() 메서드에 훈련 세트를 전달하여 점수를 출력해본다.

```python
print("test set evaluate:",knr.score(test_input, test_target))
print("train set evaluate:",knr.score(train_input, train_target))
>> test set evaluate: 0.992809406101064
>> train set evaluate: 0.9698823289099254
```

## Over-Fitting
```
train > test
```

train 세트로 모델을 평가했을때 점수가 좋았는데, test 세트에서 점수가 나쁘다면 모델이 훈련 세트에 과대적합 되었다고 말한다.
즉, 훈련 세트에만 잘 맞는 모델이라 테스트와 나중에 실전에 투입하려 새로운 샘플에 대한 예측을 만들 때 잘 동작하지 않게된다.

## Under-Fitting
```
train < test 
train , test LOW
```

반대로
- train 세트로 모델을 평가했는데 test 세트 점수가 더 높거나
- train, test 세트로 모델을 평가했는데 둘다 점수가 안좋거나
이런경우는 모델이 훈련세트에 과소 적합 되었다고 말한다.
즉, 모델이 너무 단순하여 훈련 세트에 적절히 훈련되지 않은 경우다.

원인: 훈련 세트와 테스트 세트의 크기가 매우 작기때문.

위의 경우 과소적합이므로, 이것을 해결하려면 모델을 복잡하게 만들어야하고, **훈련 세트에 더 잘맞게 만들어야한다.**

### Solution: 이웃의 개수 k 를 줄여본다
```python
knr.n_neighbors
>> 5

knr.n_neighbors = 3
knr.fit(train_input, train_target)
knr.score(train_input, train_target)
>> 0.9804899950518966

knr.score(test_input, test_target)
>> 0.9746459963987609
```

두 점수의 차이가 크지 않으므로 이 모델이 과대적합 된거같지 않기 때문에 잘 사용이 될 수 있다.


## Limitation of k-neighbor: Linear Regression
k-최근접 이웃 알고리즘은 샘플들의 무게를 평균하기 때문에 새로운 샘플이 훈련 세트의 범위를 벗어나면 엉뚱한 값을 예측한다.

50cm의 농어의 무게를 예측한다고 가정.
```python
from sklearn.linear_model import LinearRegression
lr = LinearRegression()

lr.fit(train_input, train_target)
lr.predict([[50]])
>> array([[1241.83860323]])
```

![|700](https://i.imgur.com/2m70iVY.png)
lr 객체의 coef_ 와 intercept_ 속성에 저장되어 있습니다.

```python
print(lr.coef_, lr.intercept_)
>> [[39.01714496]] [-709.01864495]
```

머신러닝에선 기울기 (gradient) 를 coefficient  라고 부르기도 하고
절편 (intercept) 를 bias 라고 부르기도 한다.

또한, 이 둘은 머신러닝 알고리즘이 찾은 값이라는 의미로 모델 파라미터라고 부른다. 흔히 머신러닝 알고리즘의 훈련 과정의 
최종 목적은 최적의 모델 파라미터를 찾는것이다 → 모델 기반 학습
모델 파라미터가 없이 훈련하면 → 사례 기반 학습


## Polynomial Regression 
위 모델을 평가해보니 과대적합.
```python
lr.score(train_input, train_target)
lr.score(test_input, test_target)
>> 0.9398463339976041
>> 0.824750312331356
```

위 모델대로라면 농어의 무게가 0g 이하로 내려가는데 현실에서는 불가능.

현실에서 농어의 길이와 무게에 대한 산점도를 자세히 보면 일직선이라기 보다 왼쪽 위로 구부러진 곡선에 가깝다. 

![|625](https://i.imgur.com/Mf8KBYh.png)


### Solution: square train and test
numpy broad-casting
```python
train_poly = np.column_stack((train_input ** 2, train_input))
test_poly = np.column_stack((test_input ** 2, test_input))

lr.fit(train_poly, train_target)
print(lr.score(train_poly, train_target),lr.score(test_poly, test_target))
>> 0.9706807451768623 0.9775935108325122
```


## Multiple Features: Multiple Regression
![|675](https://i.imgur.com/Rc8QZcq.png)

### Prepare three features
```python
perch_full = np.column_stack((perch_length, perch['Height'], perch['Width']))
perch_weight = perch['Weight']
```

### train and test
```python
from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(
    perch_full, perch_weight, random_state=42)
```

### Scikit-learn’s transformer 
특성을 만들거나 전처리하기 위해 다양한 클래스를 제공하는데 이것을 변환기라고 한다. 
LinearRegression 같은 모델 클래스는 `estimator` 라고 부르고 fit(), score(), predict() 제공
이번에 사용할 PolynomialFeatures 클래스는 `transformer` 로 fit(), transform() 제공.

예시
```python
from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures()
poly.fit([[2, 3]])
print(poly.transform([[2, 3]]))
>> [[1. 2. 3. 4. 6. 9.]]
```

fit() 메서드는 새롭게 만들 특성 조합을 찾고 transform() 메서드는 실제로 데이터를 변환한다. 
즉, 변환기는 타깃 데이터 없이 입력데이터를 변환한다.

2와 3을 각기 제곱한 4와 9가 추가되었고, 2와 3을 곱한 6은 추가되었는데, 1은 왜 추가되었을까?
![|530](https://i.imgur.com/6s9WccL.png)

선현 방정식의 절편을 항상 값이 1인 특성과 곱해지기 때문에 `include_bias=False` 로 지정하여 특성을 변환할 수 있다.

```python
poly = PolynomialFeatures(include_bias=False)
poly.fit([[2, 3]])
print(poly.transform([[2, 3]]))
>> [[2. 3. 4. 6. 9.]]
```

### prepare for train
```python
from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(include_bias=False)
poly.fit(train_input)
train_poly = poly.transform(train_input)
print(train_poly.shape)
>> (42, 9)
```

### train
```python
lr = LinearRegression()
lr.fit(train_poly, train_target)
lr.score(train_poly, train_target)
>> 0.9903557670312702
```

### more features
특성을 더 많이 추가하면 어떨까? 3제곱, 4제곱 항을 넣는것이다.
```python
poly = PolynomialFeatures(degree=5, include_bias=False)
poly.fit(train_input)
train_poly = poly.transform(train_input)
test_poly = poly.transform(test_input)
train_poly.shape
>> (42, 55)

lr.fit(train_poly, train_target)
lr.score(train_poly, train_target)
>> 0.9999999999978408

lr.fit(test_poly, test_target)
lr.score(test_poly, test_target)
>> -144.4057924
```

특성의 개수를 크게 늘리면 선형모델은 훈련 세트에 대해 거의 완벽하게 학습 하지만, 훈련 세트에 너무 과대적합 되므로 테스트 세트에서는 음수가 나올 수있다.


## Regularization
머신러닝 모델이 훈련 세트를 너무 과도하게 학습하지 못하도록 훼방하는것
즉, **모델이 훈련 세트에 과대 적합되지 않도록 만드는것.** → 선형 회귀 모델의 경우 특성에 곱해지는 계수 (또는 기울기)의 크기를 작게 만드는일.

![](https://i.imgur.com/m4fkYlX.png)

```python
from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(train_poly)
train_scaled = ss.transform(train_poly)
test_scaled = ss.transform(test_poly)
train_scaled
>> array([[ 0., -1.01339619, -1.01064448 ...
```

선형 회귀 모델에 규제를 추가한 모델을 릿지와 라쏘라고 부릅니다. 

### 릿지 회귀
**계수를 제곱한 값을 기준으로 규제를 적용**

```python
from sklearn.linear_model import Ridge
ridge = Ridge()
ridge.fit(train_scaled, train_target)
ridge.score(train_scaled, train_target)
>> 0.9858005138215177

ridge.score(test_scaled, test_target)
>> 0.9834233825354812
```

많은 특성을 사용했음에도 불구하고 훈련 세트에 너무 과대적합되지 않아 테스트 세트에서도 좋은 성능을 보여준다.

### 라쏘 회귀
라쏘는 계수의 절댓값을 기준으로 규제를 적용합니다.

```python
from sklearn.linear_model import Lasso

lasso = Lasso()
lasso.fit(train_scaled, train_target)
lasso.score(train_scaled, train_target)
>> 0.9866087194385585
```


---

# Ch.4 Classification
럭키백 예제
럭키백에 들어간 생선의 크기, 무게등이 주어졌을 때 7개의 생선에 대한 확률을 출력해야함.
## Logistic Regression (다중 분류)
로지스틱 회귀는 이름은 회귀이지만, 분류 모델입니다. 이 알고리즘은 선형 회귀와 동일하게 선형 방정식을 학습합니다. 

$$ z = a \times (Weight) + b \times (Length) + c \times (Diagonal) + d \times (Height) + e \times (Width) + f$$
여기서 abcde 는 가중치 혹은 계수입니다. 특성은 늘어났지만 전에 다뤘던 회귀를 위한 선형 방정식과 같습니다.

Z가 확률이 되려면 0~1 사이 값이 되어야합니다. 
**z가 아주 큰 음수일 때 0이 되고, z가 아주 큰 양수일때 1이 되도록 바꾸는 방법**은 시그모이드 함수를 사용하면 됩니다.

시그모이드 → 로지스틱 함수

$$ \phi = \frac{1}{1+e^{(-z)}}$$
### Data Preparation
```python
from sklearn.model_selection import train_test_split

fish_input = fish[['Weight', 'Length2', 'Length3', 'Height', "Width"]].to_numpy()
fish_target = fish['Species'].to_numpy()

train_input, test_input, train_target, test_target = train_test_split(
    fish_input, fish_target, random_state=42)
```

#### normalization
```python
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```


### Binary Classification with Logistic Regression
- bream_smelt_indexes : 도미와 빙어일 경우 True이고 그외에는 모두 False가 들어있음
- 위 배열로 train_scaled, train_target 배열에 불리언 인덱싱을 적용하여 도미와 빙어 데이터만 골라내기.

```python
bream_smelt_indexes = (train_target == 'Bream') | (train_target == 'Smelt')
train_bream_smelt = train_scaled[bream_smelt_indexes]
target_bream_smelt = train_target[bream_smelt_indexes]
```

#### predict
```python
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(train_bream_smelt, target_bream_smelt)
lr.predict(train_bream_smelt[:5])
>> array(['Bream', 'Smelt', 'Bream', 'Bream', 'Bream'], dtype=object)
```

#### Probability
- 첫번째 열은 음성 클래스 (0)에 대한 확률 → bream
- 두번째 열은 양성 클래스 (1)에 대한 확률 → smelt
```python
lr.classes_
>> array(['Bream', 'Smelt'], dtype=object)

lr.predict_proba(train_bream_smelt[:5])
>> array([[0.99760007, 0.00239993],
       [0.02737325, 0.97262675],
       [0.99486386, 0.00513614],
       [0.98585047, 0.01414953],
       [0.99767419, 0.00232581]])
```


#### evaluate with z score
- 아래 z 값을 시그모이드 함수에 적용시키면 확률을 얻을 수 있다.
- 다행히 scipy 라이브러리에도 시그모이드 함수 expit()가 있다.
```python
decisions = lr.decision_function(train_bream_smelt[:5])
>> array([-6.02991358,  3.57043428, -5.26630496, -4.24382314, -6.06135688])

from scipy.special import expit
expit(decisions)
>> array([0.00239993, 0.97262675, 0.00513614, 0.01414953, 0.00232581])
```



### Multiple Classification
- 테스트 세트의 처음 5개 샘플에 대한 예측
```python
lr = LogisticRegression(C=20, max_iter=1000)
lr.fit(train_scaled, train_target)
lr.score(train_scaled, train_target)
lr.score(test_scaled, test_target)
>> 0.93277310
>> 0.925

print(lr.predict(test_scaled[:5]))
>> ['Perch' 'Smelt' 'Pike' 'Roach' 'Perch']
```

5개 샘플에 대한 예측이므로 5개의 행이 출력됨
또한 7개의 생선에 대한 확률을 계산했으므로 7개의 열이 출력되었습니다. 
```python
lr.classes_
>> array(['Bream', 'Parkki', 'Perch', 'Pike', 'Roach', 'Smelt', 'Whitefish']

proba = lr.predict_proba(test_scaled[:5])
np.round(proba, decimals=3)
>> array([[0.   , 0.014, 0.842, 0.   , 0.135, 0.007, 0.003],
       [0.   , 0.003, 0.044, 0.   , 0.007, 0.946, 0.   ],
       [0.   , 0.   , 0.034, 0.934, 0.015, 0.016, 0.   ],
       [0.011, 0.034, 0.305, 0.006, 0.567, 0.   , 0.076],
       [0.   , 0.   , 0.904, 0.002, 0.089, 0.002, 0.001]])
```
- 첫번째 행에서 세번째 열이 0.84로 가장 높은 확률을 보여주는데, 이는 Perch를 의미

#### 선형 방정식
해당 데이터는 5개의 특성을 사용하므로 coef_ 배열의 열은 5개입니다.
그런데 인터셉트가 7개나 있습니다. 이말은 이진 분류에서 보았던 z를 7개를 계산한다는 의미입니다.
```python
print(lr.coef_.shape, lr.intercept_.shape)
>> (7, 5) (7,)
```

다중 분류는 클래스마다 z 값을 하나씩 계산한다. 그중 가장 높은 z 값을 출력하는 클래스가 예측 클래스가 된다.

→ 이진 분류에서는 시그모이드를 사용하지만, **다중 분류에서는 소프트맥스 함수를 사용하여 7개의 z값을 확률로 변환한다.**

```python
decision = lr.decision_function(test_scaled[:5])
np.round(decision, decimals=2)

from scipy.special import softmax
proba = softmax(decision, axis=1)
np.round(proba, decimals=3)
```


## Gradient Descent (경사 하강법)

### Problem
훈련한 모델을 버리지 않고 새로운 데이터에 대해서만 조금씩 더 훈련하고싶을떄 사용

### Stochastic Gradient Descent
확룔적 경사 하강법에서 확률적이란 말은 ‘무작위하게’ 혹은 ‘랜덤하게’의 기술적 표현이다. 

경사 하강법이란, 가장 가파른 경사를 따라 원하는 지점에 제일 빨리 도달하는것이 목표이다. 
하지만, 한번의 걸음이 너무 크면 산을 내려가지 못하고 오히려 올라갈 수가 있다.

1. 전체 샘플을 사용하지 않고 딱 하나의 샘플을 훈련 세트에서 랜덤하게 골라 가장 가파른 길을 찾는다.
2. 스텝을 조금씩 잡아 경사를 조금 내려간다.
3. 전체 샘플을 모두 사용 할 때 까지 반복한다.
4. 만약 정답을 찾지 못했으면, 모든 샘플을 다시 채워넣으며 다시 처음부터 시작한다.
5. 만족할만한 위치에 도달할 때 까지 계속 내려가기를 반복한다.

이렇게 훈련 세트를 한번 모두 사용하는 과정을 에포크라고 부른다.
일반적으로 경사 하강법은 수십, 수백 번 이상 에포크를 수행한다.

#### 미니 배치 & 배치 경사 하강법
![](https://i.imgur.com/YZnb2up.png)

만약 1개씩 샘플씩 선택하지 말고 무작위로 여러개의 샘플을 동시에 선택하여 경사를 따라 내려가면 minibatch gradient descent라고 한다. 

극단적으로 한번 경사로를 따라가기 위해 전체 샘플을 사용 할 수 도있다. 이를 batch gradient descent라고 한다. 전체 데이터를 사용하기 때문에 가장 안정적인 방법이 될 수 도 있다.

### Loss Function: 어디서부터 내려가야하나
가장 빠른 길을 찾아 내려가려고 하는 이 산은 무엇인가? 이 산을 바로 손실 함수라고 부른다.

loss function은 머신러닝 알고리즘이 얼마나 엉터리인지를 측정하는 기준이다. 손실 값이 작을 수를 좋으나 어떤값이 최솟값인지는 알지 못합니다. 따라서 가능한 많이 찾아보고 만족할만한 수준이라면 산을 다 내려왔다고 인정해야함.

경사 하강법에서는 손실 함수 지표를 보면서 모델 성능을 볼 수 있다.


#### Loss function 의 치명적인 단점
분류에서 손실은 정답을 못맞히는거다. 4개중 2개만 정답을 맞췄을 경우 정확도는 0.5다.
그럼 정확도를 손실 함수로 사용할 수 있을까? 

만약 그림과 같이 4개의 샘플만 있다면 가능한 정확도는 0, 0.25, 0.5, 0.75, 1 다섯가지 뿐이다.
경사 하강법 기법을 사용 할 때 아주 조금씩 내려와야하는데, 손실 함수의 범위가 커져버리면 조금씩 움직이기 힘들다.

![](https://i.imgur.com/oOO4TCY.png)

### Logistic Loss Function
![](https://i.imgur.com/qyopmYx.png)

![](https://i.imgur.com/ODq859V.png)


## SGD Classifier
확률적 경사 하강법을 사용한 분류 모델을 만들어보자.

### Prepare
```python
fish_input = fish[['Weight','Length2','Length3','Height','Width']].to_numpy()
fish_target = fish['Species'].to_numpy()

train_input, test_input, train_target, test_target = train_test_split(
    fish_input, fish_target, random_state=42)
```

```python
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```

### Train
```python
from sklearn.linear_model import SGDClassifier

sc = SGDClassifier(loss='log_loss', max_iter=10, random_state=42)
sc.fit(train_scaled, train_target)
sc.score(train_scaled, train_target)
sc.score(test_scaled, test_target)
>> 0.773109243
>> 0.775
```

위에 말한것처럼 확률적 경사 하강법은 점진적 학습이 가능하기 때문에 SDG 객체를 다시 만들지 않고 훈련한 모델 sc를 추가로 더 훈련해보죠. 

모델을 이어서 훈련할 떄는 partial_fit() 메서드를 사용한다.
```python
sc.partial_fit(train_scaled, train_target)
print(sc.score(train_scaled, train_target), sc.score(test_scaled, test_target))
>> 0.8151260504201681 0.85
```

에포크를 한번 더 실행하니 정확도가 향상되었다. 여러 에포크에서 더 훈련 해볼 필요가 있는데, 얼마나 더 훈련해야할까?


## Epoch Over/Under Fitting
확률적 경사 하강법을 사용한 모델은 에포크 횟수에 따라 과소적합이나 과대적합이 될 수 있다. 왜 이런 현상이 생길까?

에포크 횟수가 적으면 모델이 훈련 세트를 덜 학습합니다. 마치 산을 다 내려오지 못하고 훈련을 마친 셈이다.
에포크 횟수가 충분히 많으면 훈련 세트를 완전히 학습할 것입니다. 훈련 세트에 아주 잘 맞는 모델이 만들어진다.
반대로 많은 에포크 횟수 동안에 훈련한 모델은 훈련 세트에 너무 훈련되어있어 다른 테스트 세트에서는 점수가 나뻐질수있다.

![](https://i.imgur.com/QSh9H6m.png)

### epoch fitting 실험
- 300번의 에포크 동안 기록한 훈련 세트와 테스트 세트의 점수를 그려본다.
```python
import numpy as np
sc = SGDClassifier(loss='log_loss', random_state=42)
train_score, test_score = [], []
classes = np.unique(train_target)

for _ in range(0, 300):
    sc.partial_fit(train_scaled, train_target, classes=classes)
    train_score.append(sc.score(train_scaled, train_target))
    test_score.append(sc.score(test_scaled, test_target))
```

#### plot
- 데이터가 작기 때문에 아주 잘 드러나지는 않지만, 백번째 에포크 이후에는 훈련세트와 테스트 세트의 점수가 조금씩 벌어지고 있기 때문에 반복 횟수를 100에 맞추고 모델을 다시 훈련하는것이 좋다.
```python
import matplotlib.pyplot as plt

plt.plot(train_score)
plt.plot(test_score)
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.show()
```

![](https://i.imgur.com/5RPQxV5.png)

### train again with epoch
```python
from sklearn.linear_model import SGDClassifier

sc2 = SGDClassifier(loss='log_loss', max_iter=100, random_state=42)
sc2.fit(train_scaled, train_target)
sc2.score(train_scaled, train_target)
sc2.score(test_scaled, test_target)
>> 0.95
>> 0.96
```


---

# Ch.5 Decision Tree
## Decision Tree
와인 데이터셋의, 도수, 당도, pH를 가지고 레드와인(0) 과 화이트와인(1) 을 구분하는 머신러닝 프로그램을 만들어보자.

### Prepare dataset
```python
import pandas as pd
import numpy as np

wines = pd.read_csv('https://bit.ly/wine_csv_data')
wines.head(5)
wines.info()
wines.describe()
```

![|625](https://i.imgur.com/KpItuCs.png)

![|625](https://i.imgur.com/hrdRcsm.png)

![|625](https://i.imgur.com/2kimpKR.png)

#### train test split
```python
from sklearn.model_selection import train_test_split

data = wines[['alcohol', 'sugar', 'pH']].to_numpy()
target = wines['class'].to_numpy()

train_input, test_input, train_target, test_target = train_test_split(
    data, target, random_state=42)
```

#### Normalize
```python
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```

### Problem
로지스틱 회귀 분류모델을 사용하니 훈련, 테스트셋 점수가 둘다 낮게나와 모델이 과소적합된것을 확인 할 수 있다.
```python
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(train_scaled, train_target)
print(lr.score(train_scaled, train_target), lr.score(test_scaled, test_target))
>> 0.7859195402298851  0.7655384615384615
```

이 모델을 설명하기 위해 로지스틱 회귀가 학습한 coef 와 intercept를 출력해보자.

```python
print(lr.coef_, lr.intercept_)
>> [[ 0.53272315 1.67940138 -0.70910354]] [1.84709816]
```

식으로 계산하면 다음과 같다.

$$ y = Wx + b $$

$$ y = a \times (alcohol) + s \times (sugar) + p \times (pH)+ f$$

$$ y = 0.51a + 1.67s - 0.68p + 1.81 $$
이 모든 값을 더한다음, 해당 값이 0보다 크면 화이트화인, 0보다 작으면 레드와인인것을 확인할 수 있다. 그렇다면,
직감적으로 a (알고올 도수) 와 b(당도)가 높을수록 화이트와인일 가능성이 높고,
pH 농도가 높을수록 레드와인일 가능성이 높다는것을 알 수 있다.

하지만, 사실 우리는 이 모델이 왜 저런 파라미터를 학습했는지 정확히 이해하기 어렵기때문에 좀 더 간단한 방법으로 모델을 설명하고 싶을때 어떻게 해야하는지 알아보자.

### Solution
모델이 이유를 쉽게 설명하려면 대표적으로 Decision Tree를 사용한다. 결정 트리모델은 스무고개와 같습니다. 질문을 하나씩 던져서 정답과 맞춰가는 과정이다. 데이터를 잘 나눌수 있는 질문을 찾는다면 계속 질문을 추가해서 분류 정확도를 높일 수 있다.

```python
from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(random_state=42)
dt.fit(train_scaled, train_target)
print(dt.score(train_scaled, train_target))
print(dt.score(test_scaled, test_target))
>> 0.9973316912972086
>> 0.8516923076923076
```

테스트 세트 점수가 더 높게나와 과적합되었다. 트리로 출력하여 한번 봐보자.

```python

```