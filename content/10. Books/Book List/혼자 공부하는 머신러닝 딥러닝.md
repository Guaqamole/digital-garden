---
title: 혼자 공부하는 머신러닝 딥러닝
Author:
  - 박해선 저 | 한빛미디어
Status: Currently Reading
Topics: ML and DL
tags:
  - Book
Kanban-Cover: "![Book Cover|300](https://image.yes24.com/goods/96024871/XL)"
draft: false
---
# Ch.2 데이터 다루기 (classification)
## Sampling Bias
훈련 세트와 테스트 세트에 샘플이 골고루 섞여 있지 않으면 샘플링이 한쪽으로 치우칠 수 있음.
![|750](https://i.imgur.com/PJiMOIe.png)

### Problem

### Solution: Numpy
dataset: fish-market 

```python
import glob
import numpy as np
import pandas as pd

dir = glob.glob("data/fish-market/*.csv")
fish = pd.read_csv(dir[0])

bream = fish.loc[fish['Species'] == 'Bream']
smelt = fish.loc[fish['Species'] == 'Smelt']
```

![|625](https://i.imgur.com/hzzZlh0.png)

#### prepare dataset
```python
# len(bream) = 35
bream_length = bream['Length2']
bream_weight = bream['Weight']

# len(smelt) = 14
smelt_length = smelt['Length2']
smelt_weight = smelt['Weight']

length = pd.concat([bream_length, smelt_length])
weight = pd.concat([bream_weight, smelt_weight])

index = np.arange(49)
fish_input = np.column_stack([length, weight])
fish_target = np.concatenate([np.ones(35), np.zeros(14)])

>> array([[ 25.4, 242. ], [ 26.3, 290. ], [ 26.5, 340. ], [ 29. , 363. ], [ 29. , 430. ], [ 29.7, 450. ], [ 29.7, 500. ], ...
>> array([1., 1., 1., 1., ...
```

#### randomize dataset
```python
np.random.seed(42)
index = np.arange(49)
np.random.shuffle(index)
>> array([13, 45, 47, 44, 17, 27, 26, ...
```

#### np’s array indexing
```python
train_input = fish_input[index[:35]]
train_target = fish_target[index[:35]]

# 셔플된 인덱스의 값이 13이므로 인풋의 13번째 인덱스 값이 훈련 세트의 첫번째 값과 동일할것이다.
print(fish_input[13], train_input[0])
>> [ 32. 340.] [ 32. 340.]

test_input = fish_input[index[35:]]
test_target = fish_target[index[35:]]
```

#### plot
##### before
```python
import matplotlib.pylab as plt

plt.scatter(bream_length, bream_weight)
plt.scatter(smelt_length, smelt_weight)
plt.xlabel('length')
plt.xlabel('weight')
plt.show()
```
![|550](https://i.imgur.com/80zK3ie.png)

##### after
- blue: train
- orange: test
```python
import matplotlib.pylab as plt

plt.scatter(train_input[:,0], train_input[:,1])
plt.scatter(test_input[:,0], test_input[:,1])
plt.xlabel('length')
plt.xlabel('weight')
plt.show()
```

![|575](https://i.imgur.com/IOCfZbn.png)

#### Train and Predict
```python
from sklearn.neighbors import KNeighborsClassifier
kn = KNeighborsClassifier()

kn.fit(train_input, train_target)
kn.score(test_input, test_target)
kn.predict(test_input)
>> array([0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0.])
```

```python
test_target
>> array([0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0.])
```

```python
if (kn.predict(test_input) == test_target).all:
    print("Successfully Predicted.")
else:
    print("Prediction Failed")
>> Successfully Predicted.
```


## Standard Score (전처리) - The ‘z’ Score
### Problem
![|550](https://i.imgur.com/wypO5qJ.png)

→ X 축은 범위가 좁고(10 ~ 40), Y축은 범위가 넓다 (0 ~ 1000). 따라서 y축으로 조금만 멀어져도 거리가 아주 큰 값으로 계산된다. 이때문에 오른쪽위 샘플이 이웃으로 선택되지 못했다.

### Solution: Z Score
표준점수는 각 특성값이 평균에서 표준편차의 몇 배만큼 떨어져 있는지를 나타낸다. 
**이를 통해 실제 특성값의 크기와 상관없이 동일한 조건으로 비교가 가능하다.**

> [!note] Standard Score & Standard Deviation
> 분산(distribution)은 데이터에서 평균을 뺀 값을 모두 제곱한 당므 평균을 내어 구합니다. 표준 편차는 분산의 제곱근으로 데이터가 분산된 정도를 나타냅니다. 표준점수는 각 데이터가 원점에서 몇 표준편차만큼 떨어져 있는지를 나타내는 값입니다.

#### Scale
계산하는 방법은 간단합니다. 평균을 빼고 표준편차를 나누어준다.

```python
mean = np.mean(train_input, axis=0)
std = np.std(train_input, axis=0)
print(mean,std)
>> [ 28.29428571 483.35714286] [ 9.54606704 323.47456715]
```

```python
train_scaled = (train_input - mean) / std
```

![|675](https://i.imgur.com/KMZAxOJ.png)

```python
new = ([25, 150] - mean) /std
plt.scatter(train_scaled[:,0], train_scaled[:,1])
plt.scatter(new[0], new[1], marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

![|550](https://i.imgur.com/r3E3KUl.png)

#### Train
Before
```python
kn.fit(train_input, train_target)
kn.score(train_input, train_target)
kn.predict([[25,150]])

# 지표상으로는 1이 나와야하는데 0이 나옴
>> array([0.])
```

After
```python
kn_2 = KNeighborsClassifier()
kn_2.fit(train_scaled, train_target)
kn_2.score(train_scaled, train_target)
kn_2.predict([new])

# 도미로 예측 성공
>> array([1.])
```

### Conclusion
대부분의 머신러닝 알고리즘은 특성의 스케일이 다르면 잘 작동하지 않는다. 이를위해 특성을 표준 점수로 변환했다. 
스케일을 조정하는 방법은 표준점수 말고도 더 많지만 대부분의 경우 표준점수로 충분하다.

---

# Ch.3 Regression (prediction)
> [!note] 회귀란?
19세기 통계학자이자 사회학자인 프랜시스 골턴이 처음 사용했고, 키가 큰 사람의 아이가 부모보다 크지 않다는 사실을 관찰했습니다. 
이를 현상을 ‘평균으로 회귀한다’ 라고 표현하고 그 후 **두 변수 사이의 상관관계를 분석하는 방법**을 회귀라고 부르게되었다.

## Regression and Coefficient Determination (R^2)
### Dataset
```python
perch = fish.loc[fish['Species'] == 'Perch']
perch_length = perch['Length2']
perch_weight = perch['Weight']

plt.scatter(perch_length, perch_weight)
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```
![|700](https://i.imgur.com/u4wd8a1.png)

#### split train and test
```python
from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(
    perch_length, perch_weight, random_state=42)
```

이번 예제에서는 특성을 1개만 사용하므로 수동으로 2차원 배열을 만들어야한다. numpy의 reshape을 사용.
```python
test_array = np.array([1,2,3,4])
print(test_array.shape)
>> (4,)

test_array = test_array.reshape(2,2)
print(test_array.shape)
>> (2, 2)
```

### Train
```python
from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(
    perch_length, perch_weight, random_state=42)

train_input, test_input, train_target, test_target = \
np.array(train_input), np.array(test_input), np.array(train_target), np.array(test_target)
```

```python
from sklearn.neighbors import KNeighborsRegressor

train_input = train_input.reshape(-1, 1)
train_target = train_target.reshape(-1, 1)

knr = KNeighborsRegressor()
knr.fit(train_input, train_target)
knr.score(train_input, train_target)
>> 0.9698823289099254
```

### Evaluate: 결정 계수 구하기
![|775](https://i.imgur.com/1JXcVCR.png)

mae는 타깃과 예측의 절댓값 오차를 평균하여 반환한다.

```python
from sklearn.metrics import mean_absolute_error

test_input = test_input.reshape(-1, 1)
test_target = test_target.reshape(-1, 1)

test_prediction = knr.predict(test_input)
mae = mean_absolute_error(test_target, test_prediction)
mae
>> 19.157142857142862
```

결과와 예측이 평균적으로 19g 정도 타깃과 다르다는것을 알 수 있다. 

지금까지는:
train 세트를 사용해 모델을 훈련하고
test 세트를 사용해 모델을 평가했습니다.

그럼 train 세트를 사용해 평가를 해보면 어떨까? 즉, score() 메서드에 훈련 세트를 전달하여 점수를 출력해본다.

```python
print("test set evaluate:",knr.score(test_input, test_target))
print("train set evaluate:",knr.score(train_input, train_target))
>> test set evaluate: 0.992809406101064
>> train set evaluate: 0.9698823289099254
```

## Over-Fitting
```
train > test
```

train 세트로 모델을 평가했을때 점수가 좋았는데, test 세트에서 점수가 나쁘다면 모델이 훈련 세트에 과대적합 되었다고 말한다.
즉, 훈련 세트에만 잘 맞는 모델이라 테스트와 나중에 실전에 투입하려 새로운 샘플에 대한 예측을 만들 때 잘 동작하지 않게된다.

## Under-Fitting
```
train < test 
train , test LOW
```

반대로
- train 세트로 모델을 평가했는데 test 세트 점수가 더 높거나
- train, test 세트로 모델을 평가했는데 둘다 점수가 안좋거나
이런경우는 모델이 훈련세트에 과소 적합 되었다고 말한다.
즉, 모델이 너무 단순하여 훈련 세트에 적절히 훈련되지 않은 경우다.

원인: 훈련 세트와 테스트 세트의 크기가 매우 작기때문.

위의 경우 과소적합이므로, 이것을 해결하려면 모델을 복잡하게 만들어야하고, **훈련 세트에 더 잘맞게 만들어야한다.**

### Solution: 이웃의 개수 k 를 줄여본다
```python
knr.n_neighbors
>> 5

knr.n_neighbors = 3
knr.fit(train_input, train_target)
knr.score(train_input, train_target)
>> 0.9804899950518966

knr.score(test_input, test_target)
>> 0.9746459963987609
```

두 점수의 차이가 크지 않으므로 이 모델이 과대적합 된거같지 않기 때문에 잘 사용이 될 수 있다.


## Limitation of k-neighbor: Linear Regression
k-최근접 이웃 알고리즘은 샘플들의 무게를 평균하기 때문에 새로운 샘플이 훈련 세트의 범위를 벗어나면 엉뚱한 값을 예측한다.

50cm의 농어의 무게를 예측한다고 가정.
```python
from sklearn.linear_model import LinearRegression
lr = LinearRegression()

lr.fit(train_input, train_target)
lr.predict([[50]])
>> array([[1241.83860323]])
```

![|700](https://i.imgur.com/2m70iVY.png)
lr 객체의 coef_ 와 intercept_ 속성에 저장되어 있습니다.

```python
print(lr.coef_, lr.intercept_)
>> [[39.01714496]] [-709.01864495]
```

머신러닝에선 기울기 (gradient) 를 coefficient  라고 부르기도 하고
절편 (intercept) 를 bias 라고 부르기도 한다.

또한, 이 둘은 머신러닝 알고리즘이 찾은 값이라는 의미로 모델 파라미터라고 부른다. 흔히 머신러닝 알고리즘의 훈련 과정의 
최종 목적은 최적의 모델 파라미터를 찾는것이다 → 모델 기반 학습
모델 파라미터가 없이 훈련하면 → 사례 기반 학습


## Polynomial Regression 
위 모델을 평가해보니 과대적합.
```python
lr.score(train_input, train_target)
lr.score(test_input, test_target)
>> 0.9398463339976041
>> 0.824750312331356
```

위 모델대로라면 농어의 무게가 0g 이하로 내려가는데 현실에서는 불가능.

현실에서 농어의 길이와 무게에 대한 산점도를 자세히 보면 일직선이라기 보다 왼쪽 위로 구부러진 곡선에 가깝다. 

![|625](https://i.imgur.com/Mf8KBYh.png)


### Solution: square train and test
numpy broad-casting
```python
train_poly = np.column_stack((train_input ** 2, train_input))
test_poly = np.column_stack((test_input ** 2, test_input))

lr.fit(train_poly, train_target)
print(lr.score(train_poly, train_target),lr.score(test_poly, test_target))
>> 0.9706807451768623 0.9775935108325122
```


## Multiple Features: Multiple Regression
![|675](https://i.imgur.com/Rc8QZcq.png)

### Prepare three features
```python
perch_full = np.column_stack((perch_length, perch['Height'], perch['Width']))
perch_weight = perch['Weight']
```

### train and test
```python
from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(
    perch_full, perch_weight, random_state=42)
```

### Scikit-learn’s transformer 
특성을 만들거나 전처리하기 위해 다양한 클래스를 제공하는데 이것을 변환기라고 한다. 
LinearRegression 같은 모델 클래스는 `estimator` 라고 부르고 fit(), score(), predict() 제공
이번에 사용할 PolynomialFeatures 클래스는 `transformer` 로 fit(), transform() 제공.

예시
```python
from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures()
poly.fit([[2, 3]])
print(poly.transform([[2, 3]]))
>> [[1. 2. 3. 4. 6. 9.]]
```

fit() 메서드는 새롭게 만들 특성 조합을 찾고 transform() 메서드는 실제로 데이터를 변환한다. 
즉, 변환기는 타깃 데이터 없이 입력데이터를 변환한다.

2와 3을 각기 제곱한 4와 9가 추가되었고, 2와 3을 곱한 6은 추가되었는데, 1은 왜 추가되었을까?
![|530](https://i.imgur.com/6s9WccL.png)

선현 방정식의 절편을 항상 값이 1인 특성과 곱해지기 때문에 `include_bias=False` 로 지정하여 특성을 변환할 수 있다.

```python
poly = PolynomialFeatures(include_bias=False)
poly.fit([[2, 3]])
print(poly.transform([[2, 3]]))
>> [[2. 3. 4. 6. 9.]]
```

### prepare for train
```python
from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(include_bias=False)
poly.fit(train_input)
train_poly = poly.transform(train_input)
print(train_poly.shape)
>> (42, 9)
```

### train
```python
lr = LinearRegression()
lr.fit(train_poly, train_target)
lr.score(train_poly, train_target)
>> 0.9903557670312702
```

### more features
특성을 더 많이 추가하면 어떨까? 3제곱, 4제곱 항을 넣는것이다.
```python
poly = PolynomialFeatures(degree=5, include_bias=False)
poly.fit(train_input)
train_poly = poly.transform(train_input)
test_poly = poly.transform(test_input)
train_poly.shape
>> (42, 55)

lr.fit(train_poly, train_target)
lr.score(train_poly, train_target)
>> 0.9999999999978408

lr.fit(test_poly, test_target)
lr.score(test_poly, test_target)
>> -144.4057924
```

특성의 개수를 크게 늘리면 선형모델은 훈련 세트에 대해 거의 완벽하게 학습 하지만, 훈련 세트에 너무 과대적합 되므로 테스트 세트에서는 음수가 나올 수있다.


## Regularization
머신러닝 모델이 훈련 세트를 너무 과도하게 학습하지 못하도록 훼방하는것
즉, **모델이 훈련 세트에 과대 적합되지 않도록 만드는것.** → 선형 회귀 모델의 경우 특성에 곱해지는 계수 (또는 기울기)의 크기를 작게 만드는일.

![](https://i.imgur.com/m4fkYlX.png)

```python
from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(train_poly)
train_scaled = ss.transform(train_poly)
test_scaled = ss.transform(test_poly)
train_scaled
>> array([[ 0., -1.01339619, -1.01064448 ...
```

선형 회귀 모델에 규제를 추가한 모델을 릿지와 라쏘라고 부릅니다. 

### 릿지 회귀
**계수를 제곱한 값을 기준으로 규제를 적용**

```python
from sklearn.linear_model import Ridge
ridge = Ridge()
ridge.fit(train_scaled, train_target)
ridge.score(train_scaled, train_target)
>> 0.9858005138215177

ridge.score(test_scaled, test_target)
>> 0.9834233825354812
```

많은 특성을 사용했음에도 불구하고 훈련 세트에 너무 과대적합되지 않아 테스트 세트에서도 좋은 성능을 보여준다.

### 라쏘 회귀
라쏘는 계수의 절댓값을 기준으로 규제를 적용합니다.

```python
from sklearn.linear_model import Lasso

lasso = Lasso()
lasso.fit(train_scaled, train_target)
lasso.score(train_scaled, train_target)
>> 0.9866087194385585
```


---

# Ch.4 Classification
럭키백 예제
럭키백에 들어간 생선의 크기, 무게등이 주어졌을 때 7개의 생선에 대한 확률을 출력해야함.
## Logistic Regression (다중 분류)
로지스틱 회귀는 이름은 회귀이지만, 분류 모델입니다. 이 알고리즘은 선형 회귀와 동일하게 선형 방정식을 학습합니다. 

$$ z = a \times (Weight) + b \times (Length) + c \times (Diagonal) + d \times (Height) + e \times (Width) + f$$
여기서 abcde 는 가중치 혹은 계수입니다. 특성은 늘어났지만 전에 다뤘던 회귀를 위한 선형 방정식과 같습니다.

Z가 확률이 되려면 0~1 사이 값이 되어야합니다. 
**z가 아주 큰 음수일 때 0이 되고, z가 아주 큰 양수일때 1이 되도록 바꾸는 방법**은 시그모이드 함수를 사용하면 됩니다.

시그모이드 → 로지스틱 함수

$$ \phi = \frac{1}{1+e^{(-z)}}$$
### Data Preparation
```python
from sklearn.model_selection import train_test_split

fish_input = fish[['Weight', 'Length2', 'Length3', 'Height', "Width"]].to_numpy()
fish_target = fish['Species'].to_numpy()

train_input, test_input, train_target, test_target = train_test_split(
    fish_input, fish_target, random_state=42)
```

#### normalization
```python
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```


### Binary Classification with Logistic Regression
- bream_smelt_indexes : 도미와 빙어일 경우 True이고 그외에는 모두 False가 들어있음
- 위 배열로 train_scaled, train_target 배열에 불리언 인덱싱을 적용하여 도미와 빙어 데이터만 골라내기.

```python
bream_smelt_indexes = (train_target == 'Bream') | (train_target == 'Smelt')
train_bream_smelt = train_scaled[bream_smelt_indexes]
target_bream_smelt = train_target[bream_smelt_indexes]
```

#### predict
```python
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(train_bream_smelt, target_bream_smelt)
lr.predict(train_bream_smelt[:5])
>> array(['Bream', 'Smelt', 'Bream', 'Bream', 'Bream'], dtype=object)
```

#### Probability
- 첫번째 열은 음성 클래스 (0)에 대한 확률 → bream
- 두번째 열은 양성 클래스 (1)에 대한 확률 → smelt
```python
lr.classes_
>> array(['Bream', 'Smelt'], dtype=object)

lr.predict_proba(train_bream_smelt[:5])
>> array([[0.99760007, 0.00239993],
       [0.02737325, 0.97262675],
       [0.99486386, 0.00513614],
       [0.98585047, 0.01414953],
       [0.99767419, 0.00232581]])
```


#### evaluate with z score
- 아래 z 값을 시그모이드 함수에 적용시키면 확률을 얻을 수 있다.
- 다행히 scipy 라이브러리에도 시그모이드 함수 expit()가 있다.
```python
decisions = lr.decision_function(train_bream_smelt[:5])
>> array([-6.02991358,  3.57043428, -5.26630496, -4.24382314, -6.06135688])

from scipy.special import expit
expit(decisions)
>> array([0.00239993, 0.97262675, 0.00513614, 0.01414953, 0.00232581])
```



### Multiple Classification
- 테스트 세트의 처음 5개 샘플에 대한 예측
```python
lr = LogisticRegression(C=20, max_iter=1000)
lr.fit(train_scaled, train_target)
lr.score(train_scaled, train_target)
lr.score(test_scaled, test_target)
>> 0.93277310
>> 0.925

print(lr.predict(test_scaled[:5]))
>> ['Perch' 'Smelt' 'Pike' 'Roach' 'Perch']
```

5개 샘플에 대한 예측이므로 5개의 행이 출력됨
또한 7개의 생선에 대한 확률을 계산했으므로 7개의 열이 출력되었습니다. 
```python
lr.classes_
>> array(['Bream', 'Parkki', 'Perch', 'Pike', 'Roach', 'Smelt', 'Whitefish']

proba = lr.predict_proba(test_scaled[:5])
np.round(proba, decimals=3)
>> array([[0.   , 0.014, 0.842, 0.   , 0.135, 0.007, 0.003],
       [0.   , 0.003, 0.044, 0.   , 0.007, 0.946, 0.   ],
       [0.   , 0.   , 0.034, 0.934, 0.015, 0.016, 0.   ],
       [0.011, 0.034, 0.305, 0.006, 0.567, 0.   , 0.076],
       [0.   , 0.   , 0.904, 0.002, 0.089, 0.002, 0.001]])
```
- 첫번째 행에서 세번째 열이 0.84로 가장 높은 확률을 보여주는데, 이는 Perch를 의미

#### 선형 방정식
해당 데이터는 5개의 특성을 사용하므로 coef_ 배열의 열은 5개입니다.
그런데 인터셉트가 7개나 있습니다. 이말은 이진 분류에서 보았던 z를 7개를 계산한다는 의미입니다.
```python
print(lr.coef_.shape, lr.intercept_.shape)
>> (7, 5) (7,)
```

다중 분류는 클래스마다 z 값을 하나씩 계산한다. 그중 가장 높은 z 값을 출력하는 클래스가 예측 클래스가 된다.

→ 이진 분류에서는 시그모이드를 사용하지만, **다중 분류에서는 소프트맥스 함수를 사용하여 7개의 z값을 확률로 변환한다.**

```python
decision = lr.decision_function(test_scaled[:5])
np.round(decision, decimals=2)

from scipy.special import softmax
proba = softmax(decision, axis=1)
np.round(proba, decimals=3)
```


## Gradient Descent (경사 하강법)

### Problem
훈련한 모델을 버리지 않고 새로운 데이터에 대해서만 조금씩 더 훈련하고싶을떄 사용

### Stochastic Gradient Descent
확룔적 경사 하강법에서 확률적이란 말은 ‘무작위하게’ 혹은 ‘랜덤하게’의 기술적 표현이다. 

경사 하강법이란, 가장 가파른 경사를 따라 원하는 지점에 제일 빨리 도달하는것이 목표이다. 
하지만, 한번의 걸음이 너무 크면 산을 내려가지 못하고 오히려 올라갈 수가 있다.

1. 전체 샘플을 사용하지 않고 딱 하나의 샘플을 훈련 세트에서 랜덤하게 골라 가장 가파른 길을 찾는다.
2. 스텝을 조금씩 잡아 경사를 조금 내려간다.
3. 전체 샘플을 모두 사용 할 때 까지 반복한다.
4. 만약 정답을 찾지 못했으면, 모든 샘플을 다시 채워넣으며 다시 처음부터 시작한다.
5. 만족할만한 위치에 도달할 때 까지 계속 내려가기를 반복한다.

이렇게 훈련 세트를 한번 모두 사용하는 과정을 에포크라고 부른다.
일반적으로 경사 하강법은 수십, 수백 번 이상 에포크를 수행한다.

#### 미니 배치 & 배치 경사 하강법
![](https://i.imgur.com/YZnb2up.png)

만약 1개씩 샘플씩 선택하지 말고 무작위로 여러개의 샘플을 동시에 선택하여 경사를 따라 내려가면 minibatch gradient descent라고 한다. 

극단적으로 한번 경사로를 따라가기 위해 전체 샘플을 사용 할 수 도있다. 이를 batch gradient descent라고 한다. 전체 데이터를 사용하기 때문에 가장 안정적인 방법이 될 수 도 있다.

### Loss Function: 어디서부터 내려가야하나
가장 빠른 길을 찾아 내려가려고 하는 이 산은 무엇인가? 이 산을 바로 손실 함수라고 부른다.

loss function은 머신러닝 알고리즘이 얼마나 엉터리인지를 측정하는 기준이다. 손실 값이 작을 수를 좋으나 어떤값이 최솟값인지는 알지 못합니다. 따라서 가능한 많이 찾아보고 만족할만한 수준이라면 산을 다 내려왔다고 인정해야함.

경사 하강법에서는 손실 함수 지표를 보면서 모델 성능을 볼 수 있다.


#### Loss function 의 치명적인 단점
분류에서 손실은 정답을 못맞히는거다. 4개중 2개만 정답을 맞췄을 경우 정확도는 0.5다.
그럼 정확도를 손실 함수로 사용할 수 있을까? 

만약 그림과 같이 4개의 샘플만 있다면 가능한 정확도는 0, 0.25, 0.5, 0.75, 1 다섯가지 뿐이다.
경사 하강법 기법을 사용 할 때 아주 조금씩 내려와야하는데, 손실 함수의 범위가 커져버리면 조금씩 움직이기 힘들다.

![](https://i.imgur.com/oOO4TCY.png)

### Logistic Loss Function
![](https://i.imgur.com/qyopmYx.png)

![](https://i.imgur.com/ODq859V.png)


## SGD Classifier
확률적 경사 하강법을 사용한 분류 모델을 만들어보자.

### Prepare
```python
fish_input = fish[['Weight','Length2','Length3','Height','Width']].to_numpy()
fish_target = fish['Species'].to_numpy()

train_input, test_input, train_target, test_target = train_test_split(
    fish_input, fish_target, random_state=42)
```

```python
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```

### Train
```python
from sklearn.linear_model import SGDClassifier

sc = SGDClassifier(loss='log_loss', max_iter=10, random_state=42)
sc.fit(train_scaled, train_target)
sc.score(train_scaled, train_target)
sc.score(test_scaled, test_target)
>> 0.773109243
>> 0.775
```

위에 말한것처럼 확률적 경사 하강법은 점진적 학습이 가능하기 때문에 SDG 객체를 다시 만들지 않고 훈련한 모델 sc를 추가로 더 훈련해보죠. 

모델을 이어서 훈련할 떄는 partial_fit() 메서드를 사용한다.
```python
sc.partial_fit(train_scaled, train_target)
print(sc.score(train_scaled, train_target), sc.score(test_scaled, test_target))
>> 0.8151260504201681 0.85
```

에포크를 한번 더 실행하니 정확도가 향상되었다. 여러 에포크에서 더 훈련 해볼 필요가 있는데, 얼마나 더 훈련해야할까?


## Epoch Over/Under Fitting
확률적 경사 하강법을 사용한 모델은 에포크 횟수에 따라 과소적합이나 과대적합이 될 수 있다. 왜 이런 현상이 생길까?

에포크 횟수가 적으면 모델이 훈련 세트를 덜 학습합니다. 마치 산을 다 내려오지 못하고 훈련을 마친 셈이다.
에포크 횟수가 충분히 많으면 훈련 세트를 완전히 학습할 것입니다. 훈련 세트에 아주 잘 맞는 모델이 만들어진다.
반대로 많은 에포크 횟수 동안에 훈련한 모델은 훈련 세트에 너무 훈련되어있어 다른 테스트 세트에서는 점수가 나뻐질수있다.

![](https://i.imgur.com/QSh9H6m.png)

### epoch fitting 실험
- 300번의 에포크 동안 기록한 훈련 세트와 테스트 세트의 점수를 그려본다.
```python
import numpy as np
sc = SGDClassifier(loss='log_loss', random_state=42)
train_score, test_score = [], []
classes = np.unique(train_target)

for _ in range(0, 300):
    sc.partial_fit(train_scaled, train_target, classes=classes)
    train_score.append(sc.score(train_scaled, train_target))
    test_score.append(sc.score(test_scaled, test_target))
```

#### plot
- 데이터가 작기 때문에 아주 잘 드러나지는 않지만, 백번째 에포크 이후에는 훈련세트와 테스트 세트의 점수가 조금씩 벌어지고 있기 때문에 반복 횟수를 100에 맞추고 모델을 다시 훈련하는것이 좋다.
```python
import matplotlib.pyplot as plt

plt.plot(train_score)
plt.plot(test_score)
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.show()
```

![](https://i.imgur.com/5RPQxV5.png)

### train again with epoch
```python
from sklearn.linear_model import SGDClassifier

sc2 = SGDClassifier(loss='log_loss', max_iter=100, random_state=42)
sc2.fit(train_scaled, train_target)
sc2.score(train_scaled, train_target)
sc2.score(test_scaled, test_target)
>> 0.95
>> 0.96
```


---

# Ch.5 Decision Tree
## Decision Tree
와인 데이터셋의, 도수, 당도, pH를 가지고 레드와인(0) 과 화이트와인(1) 을 구분하는 머신러닝 프로그램을 만들어보자.

### Prepare dataset
```python
import pandas as pd
import numpy as np

wines = pd.read_csv('https://bit.ly/wine_csv_data')
wines.head(5)
wines.info()
wines.describe()
```

![|625](https://i.imgur.com/KpItuCs.png)

![|625](https://i.imgur.com/hrdRcsm.png)

![|625](https://i.imgur.com/2kimpKR.png)

#### train test split
```python
from sklearn.model_selection import train_test_split

data = wines[['alcohol', 'sugar', 'pH']].to_numpy()
target = wines['class'].to_numpy()

train_input, test_input, train_target, test_target = train_test_split(
    data, target, random_state=42)
```

#### Normalize
```python
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```

### Problem
로지스틱 회귀 분류모델을 사용하니 훈련, 테스트셋 점수가 둘다 낮게나와 모델이 과소적합된것을 확인 할 수 있다.
```python
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(train_scaled, train_target)
print(lr.score(train_scaled, train_target), lr.score(test_scaled, test_target))
>> 0.7859195402298851  0.7655384615384615
```

이 모델을 설명하기 위해 로지스틱 회귀가 학습한 coef 와 intercept를 출력해보자.

```python
print(lr.coef_, lr.intercept_)
>> [[ 0.53272315 1.67940138 -0.70910354]] [1.84709816]
```

식으로 계산하면 다음과 같다.

$$ y = Wx + b $$

$$ y = a \times (alcohol) + s \times (sugar) + p \times (pH)+ f$$

$$ y = 0.51a + 1.67s - 0.68p + 1.81 $$
이 모든 값을 더한다음, 해당 값이 0보다 크면 화이트화인, 0보다 작으면 레드와인인것을 확인할 수 있다. 그렇다면,
직감적으로 a (알고올 도수) 와 b(당도)가 높을수록 화이트와인일 가능성이 높고,
pH 농도가 높을수록 레드와인일 가능성이 높다는것을 알 수 있다.

하지만, 사실 우리는 이 모델이 왜 저런 파라미터를 학습했는지 정확히 이해하기 어렵기때문에 좀 더 간단한 방법으로 모델을 설명하고 싶을때 어떻게 해야하는지 알아보자.

### Solution
모델이 이유를 쉽게 설명하려면 대표적으로 Decision Tree를 사용한다. 결정 트리모델은 스무고개와 같습니다. 질문을 하나씩 던져서 정답과 맞춰가는 과정이다. 데이터를 잘 나눌수 있는 질문을 찾는다면 계속 질문을 추가해서 분류 정확도를 높일 수 있다.

```python
from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(random_state=42)
dt.fit(train_scaled, train_target)
print(dt.score(train_scaled, train_target))
print(dt.score(test_scaled, test_target))
>> 0.9973316912972086
>> 0.8516923076923076
```

테스트 세트 점수가 더 높게나와 과적합되었다. 트리로 출력하여 한번 봐보자.

```python
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree

plt.figure(figsize=(10,7))
plot_tree(dt, max_depth=1, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
plt.show()
```
![|625](https://i.imgur.com/vPb0imC.png)

#### Tree Explanation
![|447](https://i.imgur.com/sLezCck.png)

루트노드는 sugar가 -0.239 이하인지 질문한다. 만약 어떤 샘플의 당도가 -0.239와 같거나 작으면 왼쪽가지로 이동한다.
**즉, 왼쪽이 Yes, 오른쪽이 No 입니다.**

루트노드
- 샘플수 : 4872
- 음성클래스 (레드와인) : 1165
- 양성클래스 (화이트와인) : 3707

왼쪽노드 (당도가 더 낮은지 질문함)
- 샘플수 : 2690
- 음성클래스: 1080
- 양성클래스 : 1610
→ 루트노드보다 양성 클래스 비율이 크게 줄어들었는데 이유는 오른쪽 노드 때문이다.

오른쪽 노드 (대부분 양성클래스가 이동함)
- 특징: 색깔이 루트노드보다 더 진하다. 이는 filled=True 로 지정했기 때문.

결정 트리에서 예측하는 방법은 간단합니다. **리프 노드에서 가장 많은 클래스가 예측 클래스가 됩니다.** 만약, 이 결정 트리의 성장을 여기서 몀춘다면 왼쪽 노드에 도달한 샘플과 오른쪽 노드에 도달한 샘플은 모두 양성클래스로 예측된다.


## Gini Impurity (불순도)
앞의 그린 트리에서 루트노드는 어떻게 당도 -0.239를 기준으로 왼쪽과 오른쪽을 나누었을까? 바로 criterion 매개변수에 지정한 지니 불순도를 사용하기 때문이다.

DecisionTreeClassifier 클래스의 criterion 매개변수 기본값은 gini 이다. criterion 매개변수의 용도는 노드에서 데이터를 분할할 기준을 정하는것이다.

지니 불순도는 클래스의 비율을 제곱해서 더한다음 1에서 뺍니다:
$$ gini = 1- (negative^{2} + positive^2) $$
$$ gini = 1 - ((\frac{1165}{4872})^{2} + (\frac{3707}{4872})^2) $$
최악: 100개의 샘플이 있는 어떤 노드의 두 클래스 비율이 정확히 1/2 이라서 지니 불순도가 0.5가 되는 상황
순수노드: 노드에 하나의 클래스만있어서 지니 불순도는 0이됨

→ **결정 트리 모델은 부모노드와 자식노드의 불순도 차이가 가능한 크도록 트리를 성장시킨다.**
이렇게 **부모와 자식노드 사이의 불순도 차이를 정보 이득** 이라고 부르며, 최종 목표는 정보 이득이 최대가 되도록 하는것이다.

### 가지치기
불순도 기준을 사용해 정보 이득이 최고가 되도록 노드를 분할하는데, 노드를 순수하게 나눌수록 정보 이득이 커진다. 새로운 샘플에 대해 예측할 때 에는 노드의 질문에 따라 트리를 이동한다. 그리고 마지막에 도달한 노드의 클래스 비율을 보고 예측을 만든다.

하지만, 앞의 트리는 제한 없이 자라났기 때문에 훈련 세트보다 테스트 세트에서 점수가 크게 낮았습니다. 따라서 가지치기를 통해 트리가 무작정 끝까지 자라나는것을 제한해야한다.

그럼 어떻게 가지치기를 하나? 여러가지 방법이 있다.
#### 1. 트리의 최대 깊이를 지정
```python
dt = DecisionTreeClassifier(max_depth=3, random_state=42)
dt.fit(train_scaled, train_target)
print(dt.score(train_scaled, train_target), dt.score(test_scaled, test_target))
>> 0.8499589490968801 0.8363076923076923
```

훈련 세트의 성능은 낮아졌지만, 테스트 세트의 성능은 거의 그대로다. plot_tree로 확인해보자.
```python
plt.figure(figsize=(20,15))
plot_tree(dt, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
plt.show()
```

![](https://i.imgur.com/lyhpkAZ.png)

여기서 우리가 알 수있는것은:
- 깊이 1의 노드는 모두 sugar를 기준으로 훈련 세트를 나누지만 깊이 2의 노드는 맨 왼쪽만 당도를 기준으로 나누고 나머지는 알코올과 ph를 사용한것을 볼 수 있다.
- 깊이 3(리프)노드를 보면, 왼쪽에서 세번째에 있는 노드만 음성 클래스가 더 많다. 즉, 이 노드에 도착해야만 레드 와인으로 예측합니다. 그럼, 루트노드부터 이 노드까지 도달하려면 당도는 -0.239 보다 작고 또 -0.802보다 커야한다. 
- 결론: 당도가 -0.802 보다 크고 -0.239 보다 작은 와인중에 알코올 도수가 0.454와 같거나 작은것이 바로 레드와인이다.
- 물론, 트리의 깊이가 비교적 얼마 되지 않아서 해석이 쉽지만, 실전에선 많은 특성을 사용하고 트리의 깊이도 깊이지기 떄문에 해석이 쉽진않다.
- 특성갚의 스케일은 결정 트리 알고리즘에 아무런 영향을 미치지 않기 때문에 표준화 전처리를 할 필요가 없다.

### Feature Importance
결정 트리는 어떤 특성이 가장 유용한지 나타내는 특성 중요도를 계산해준다.
```python
dt.feature_importances_
>> array([0.12871631, 0.86213285, 0.00915084])
```

역시 두번째 특성인 당도가 0.87 정도로 특성 중요도가 가장 높은것을 확인 할 수 있으며, 그다음 알코올 도수, pH 순이다.

## Cross-validation & Grid Search
지금까지 우리는 train 세트에서 모델을 훈련하고 테스트 세트에서 모델을 평가했다. 하지만 테스트 세트를 사용해 자꾸 성능을 확인하다 보면 점점 테스트 세트에 모델을 맞추게 되는셈이다.

**따라서, 테스트 세트로 일반화 성능을 올바르게 예측하려면 가능한 한 테스트 세트를 사용하지 말아야한다.** 모델을 만들고나서, 마지막에 딱 한번만 사용하는것이 좋다. 그렇다면. max_depth 매개변수를 사용한 하이퍼 파라미터 튜닝을 어떻게 할 수 있을까?

### Validaton Set
테스트 세트를 사용하지 않으면 모델이 과대 적합인지 과소적합인지 판단하기 어렵다. 테스트 세트를 사용하지 않고 이를 측정하려면 train set를 또 나눌 수 있다. 이 데이터를 검증 세트라고한다.

![|568](https://i.imgur.com/0mOIzFK.png)

>[!note] 얼마나 많은 샘플을 나눠야하는가?
>보통 20 - 30% 를 테스트 세트와 검증 세트로 떼어놓지만, 
>훈련 데이터가 아주 많다면 단 몇 %만 떼어 놓아도 전체 데이터를 대표하는 데 문제가 없다.

```python
from sklearn.model_selection import train_test_split

data = wines[['alcohol', 'sugar', 'pH']].to_numpy()
target = wines['class'].to_numpy()

train_input, test_input, train_target, test_target = train_test_split(
    data, target, test_size=0.2, random_state=42)
```

그 다음 `train_input` 과 `train_target` 을 다시 train_test_split 함수에 넣어,
- 훈련 세트 `sub_input` , `sub_target`
- 검증 세트 `val_input`, `val_target` 
을 만든다. test_size 매개변수를 0.2로 지정하여 train_input의 약 20%를 val_input으로 만든다.
```python
sub_input, val_input, sub_target, val_target = train_test_split(train_input, train_target, test_size=0.2, random_state=42)

print(sub_input.shape, val_input.shape)
>> (4157, 3) (1040, 3)
```

#### train with validation set
```python
from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(random_state=42)
dt.fit(sub_input, sub_target)
print(dt.score(sub_input, sub_target))
print(dt.score(val_input, val_target))
>> 0.9971133028626413 0.864423076923077
```

확인해보면 해당 모델은 확실히 훈련 세트에 과적합되어있는것을 알 수 있다. 이런 경우는 매개변수를 바꿔서 더 좋은 모델을 찾아야한다.

### Cross-validation
보통 많은 데이터를 훈련에 사용 할 수록 좋은 모델이 만들어진다. 그렇다고 검증 세트를 너무 조금 떼어놓으면 검증 점수가 들쭉날쭉하고 불안정해진다. 이럴때 바로 교차검증을 이용하면 안정적인 검증 점수를 얻고 훈련에 더 많은 데이터를 사용할 수 있게된다.

![|625](https://i.imgur.com/JLhnK9d.png)

이해를 돕기 위해 3 폴드 교차 검증을 예시로 들었지만, 실제로는 5, 10 폴드 교차 검증을 많이 사용한다. 이렇게 할 경우 데이터의 80-90%까지 훈련에 사용 할 수 있게된다. 

```python
from sklearn.model_selection import cross_validate
scores = cross_validate(dt, train_input, train_target)
print(scores)

{
 'fit_time': array([0.00749016, 0.00693107, 0.00626993, 0.00561309, 0.00528789]), 
 'score_time': array([0.00138688, 0.00117397, 0.00095272, 0.0010078 , 0.00090909]), 
 'test_score': array([0.87019231, 0.84615385, 0.87680462, 0.84889317, 0.83541867])
 }
```

- 처음 2개의 키는 각각 모델을 훈련하는 시간과 검증하는 시간을 의미한다.
- 각 키마다 5개의 숫자가 담겨있다. cross_validate() 함수는 기본적으로 5폴드 교차 검증을 수행.
- 교차 검증의 최종 점수는 test_score 키에 담긴 5개의 점수를 평균하여 얻을 수 있다.
- **이름은 test_score 지만, 검증 폴드의 점수이다**. 혼동하지 말자.
```python
import numpy as np
print(np.mean(scores['test_score']))
>> 0.8554925223957948
```

### splitter
한가지 주의 할 점은 cross_validate() 는 훈련 세트를 섞어 폴드를 나누지 않습니다. 앞서 우리는 스플릿 함수로 전체 데이터를 섞은 후, 훈련 세트를 준비 했기 떄문에 따로 섞을 필요가 없었지만, 만약 교차 검증을 할 때 훈련 세트를 섞으려면 splitter 를 지정해야합니다.

사이킷런의 분할기는 교차 검증에서 폴드를 어떻게 나눌지 결정한다.
#### KFold
기본적으로 회귀 모델일 경우, KFold 분할기를 사용하고 
#### Stratified KFold
분류 모델일 경우 타깃 클래스를 골고루 나누기 위해 S Kfold를 사용.
```python
from sklearn.model_selection import StratifiedKFold
splitter = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
scores = cross_validate(dt, train_input, train_target, cv=splitter)
np.mean(scores['test_score'])
>> 0.8581873425226026
```

## Hyper-Parameter Tuning
모델 파라미터: 머신러닝 모델이 학습하는 파라미터
하이퍼 파라미터: 모델이 학습할 수 없어서 사용자가 지정해야하만 하는 파라미터
사이킷런과 같은 머신러닝 라이브러리를 사용할 때 이런 하이퍼 파라미터는 모두 클래스나 메서드의 매개변수로 표현된다.

순서:
1. 라이브러리가 제공하는 기본값을 그대로 사용해 모델을 훈련
2. 검증 세트의 점수나 교차 검증을 통해서 매개변수를 조금씩 바꿔본다.
3. 모델마다 적게는 1~2개에서, 많게는 5~6개의 매개변수를 제공
4. 매개변수를 바꿔가면서 모델을 훈련하고 교차 검증을 수행

여기서 아주 중요한 개념이 나온다. 결정 트리 모델에서 최적의 `max_depth` 값을 찾았다고 가정해보자. 
그다음 max_depth를 최적의 값으로 고정하고 min_samples_split을 바꿔가며 최적의 값을 찾았다.

그럼, 이렇게 한 매개변수의 최적값을 찾고 다른 매개변수의 최적값을 찾아도 될까? NO.
불행하게도 max_depth 의 최적값은 min_samples_split 매개변수의 값이 바뀌면 함께 달라진다.
즉, 이 두 매개변수를 동시에 바꿔가며 최적의 값을 찾아야 하는거다!

### Grid Search
게다가 매개변수가 많아지면 더 복잡해지기 때문에, 사이킷런에서 제공하는 grid search를 이용한다.
사이킷런의 `GridSearchCV` 클래스는 친절하게도 하이퍼파라미터 탐색과, 교차 검증을 한 번에 수행 할 수 있다.
따라서, 별도로 `cross_validate()` 함수를 호출할 필요가 없다.

기본 매개변수를 사용한 결정 트리 모델에서 `min_impurity_decrease` 매개변수의 최적값을 찾아보자.
여기서는 0.0001 부터 0.0005 까지 0.0001씩 증가하는 5개의 값을 시도하겠다.
```python
from sklearn.model_selection import GridSearchCV
params = {'min_impurity_decrease': [0.0001, 0.0002, 0.0003, 0.0004, 0.0005]}
```

cv 매개변수 기본값은 5이기 때문에, min_impurity_decrease 값마다 5폴드 교차 검증을 수행하게된다. 결국 5 * 5  = 25 개의 모델을 훈련한다.
많은 모델을 훈련하기 때문에 -1로 지정하여 시스템에 있는 가능한 모든 코어를 사용하자.

```python
gs = GridSearchCV(DecisionTreeClassifier(random_state=42), params, n_jobs=-1)
gs.fit(train_input, train_target)
```

교차 검증에서 최적의 하이퍼 파라미터를 찾으면 전체 훈련 세트로 모델을 다시 만들어야 한다고했다. 
아주 편리하게도, 사이킷런의 그리드 서치는 훈련이 끝나면 25개의 모델중에서 검증 점수가 가장 높은 모델의 매개변수 조합으로
전체 훈련 세트에서 자동으로 다시 모델을 훈련한다. 이 모델은 gs 객체의 `best_estimator_` 속성에 저장되어있다.

```python
dt = gs.best_estimator_
dt.score(train_input, train_target)
>> 0.9615162593804117

print(gs.best_params_)
>> {'min_impurity_decrease': 0.0001}
```

각 매개변수에서 수행한 교차 검증의 평균 점수는 `cv_results_` 의 `mean_test_score` 키에 저장되어있다.
```python
print(gs.cv_results_['mean_test_score'])
>> [0.86800067 0.86453617 0.86492226 0.86780891 0.86761605]

best_index = np.argmax(gs.cv_results_['mean_test_score'])
print(gs.cv_results_['params'][best_index])
>> {'min_impurity_decrease': 0.0001}
```

최상의 검증 점수를 만든 매개변수 조합이며, 앞에서 출력한 gs.best_params_ 와 동일한것을 보아, 최적의 파라미터라고 생각할수있다.

### Random Search
랜덤 서치는 교차검증으로 랜덤한 하이퍼 파라미터 탐색을 수행한다. 최정상의 모델을 찾은 후 훈련 세트 전체를 사용해 최종 모델을 훈련한다.

첫번째 매개변수로 그리드 서치를 수행할 모델 객체를 전달한다.
두번째 매개변수에는 탐색할 모델의 매개변수와 확률 분호 객체를 전달한다.

scoring, cv, n_jobs, return_train_score 매개변수는 cross_validate() 함수와 동일하다.

## Ensenble

## Random Forest

