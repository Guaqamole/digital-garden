---
title: 혼자 공부하는 머신러닝 딥러닝
Author:
  - 박해선 저 | 한빛미디어
Status: Currently Reading
Topics: ML and DL
tags:
  - Book
Kanban-Cover: "![Book Cover|300](https://image.yes24.com/goods/96024871/XL)"
draft: false
---
# Ch.2 데이터 다루기
## Sampling Bias
훈련 세트와 테스트 세트에 샘플이 골고루 섞여 있지 않으면 샘플링이 한쪽으로 치우칠 수 있음.
![|750](https://i.imgur.com/PJiMOIe.png)

### Problem

### Solution: Numpy
dataset: fish-market 

```python
import glob
import numpy as np
import pandas as pd

dir = glob.glob("data/fish-market/*.csv")
fish = pd.read_csv(dir[0])

bream = fish.loc[fish['Species'] == 'Bream']
smelt = fish.loc[fish['Species'] == 'Smelt']
```

![|625](https://i.imgur.com/hzzZlh0.png)

#### prepare dataset
```python
# len(bream) = 35
bream_length = bream['Length2']
bream_weight = bream['Weight']

# len(smelt) = 14
smelt_length = smelt['Length2']
smelt_weight = smelt['Weight']

length = pd.concat([bream_length, smelt_length])
weight = pd.concat([bream_weight, smelt_weight])

index = np.arange(49)
fish_input = np.column_stack([length, weight])
fish_target = np.concatenate([np.ones(35), np.zeros(14)])

>> array([[ 25.4, 242. ], [ 26.3, 290. ], [ 26.5, 340. ], [ 29. , 363. ], [ 29. , 430. ], [ 29.7, 450. ], [ 29.7, 500. ], ...
>> array([1., 1., 1., 1., ...
```

#### randomize dataset
```python
np.random.seed(42)
index = np.arange(49)
np.random.shuffle(index)
>> array([13, 45, 47, 44, 17, 27, 26, ...
```

#### np’s array indexing
```python
train_input = fish_input[index[:35]]
train_target = fish_target[index[:35]]

# 셔플된 인덱스의 값이 13이므로 인풋의 13번째 인덱스 값이 훈련 세트의 첫번째 값과 동일할것이다.
print(fish_input[13], train_input[0])
>> [ 32. 340.] [ 32. 340.]

test_input = fish_input[index[35:]]
test_target = fish_target[index[35:]]
```

#### plot
##### before
```python
import matplotlib.pylab as plt

plt.scatter(bream_length, bream_weight)
plt.scatter(smelt_length, smelt_weight)
plt.xlabel('length')
plt.xlabel('weight')
plt.show()
```
![|550](https://i.imgur.com/80zK3ie.png)

##### after
- blue: train
- orange: test
```python
import matplotlib.pylab as plt

plt.scatter(train_input[:,0], train_input[:,1])
plt.scatter(test_input[:,0], test_input[:,1])
plt.xlabel('length')
plt.xlabel('weight')
plt.show()
```

![|575](https://i.imgur.com/IOCfZbn.png)

#### Train and Predict
```python
from sklearn.neighbors import KNeighborsClassifier
kn = KNeighborsClassifier()

kn.fit(train_input, train_target)
kn.score(test_input, test_target)
kn.predict(test_input)
>> array([0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0.])
```

```python
test_target
>> array([0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0.])
```

```python
if (kn.predict(test_input) == test_target).all:
    print("Successfully Predicted.")
else:
    print("Prediction Failed")
>> Successfully Predicted.
```


## Standard Score (전처리) - The ‘z’ Score
### Problem
![|550](https://i.imgur.com/wypO5qJ.png)

→ X 축은 범위가 좁고(10 ~ 40), Y축은 범위가 넓다 (0 ~ 1000). 따라서 y축으로 조금만 멀어져도 거리가 아주 큰 값으로 계산된다. 이때문에 오른쪽위 샘플이 이웃으로 선택되지 못했다.

### Solution: Z Score
표준점수는 각 특성값이 평균에서 표준편차의 몇 배만큼 떨어져 있는지를 나타낸다. 
**이를 통해 실제 특성값의 크기와 상관없이 동일한 조건으로 비교가 가능하다.**

> [!note] Standard Score & Standard Deviation
> 분산(distribution)은 데이터에서 평균을 뺀 값을 모두 제곱한 당므 평균을 내어 구합니다. 표준 편차는 분산의 제곱근으로 데이터가 분산된 정도를 나타냅니다. 표준점수는 각 데이터가 원점에서 몇 표준편차만큼 떨어져 있는지를 나타내는 값입니다.

계산하는 방법은 간단합니다. 평균을 빼고 표준편차를 나누어준다.

```python
mean = np.mean(train_input, axis=0)
std = np.std(train_input, axis=0)
print(mean,std)
>> [ 28.29428571 483.35714286] [ 9.54606704 323.47456715]
```

```python
train_scaled = (train_input - mean) / std
```

![|675](https://i.imgur.com/KMZAxOJ.png)

