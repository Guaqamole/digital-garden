---
title: 퍼셉트론 (Perceptron)
date: 2024-04-30
draft: false
tags:
  - DeepLearning
  - Concept
complete: true
---
## Perceptron
'인공뉴런' :  ' 가 중 치' 와 ' 편 향' 을 매 개 변 수 로 설정한 입출력을 갖춘 알고리즘이다!
퍼셉트론은다수의신호를입력으로받아하나의신호를출력합니다. 여기서말하는신호란 전류나강물처럼흐름이있는것을상상하면좋습니다. 전류가전선을타고흐르는전자를내보 내듯, 퍼셉트론신호도흐름을만들고정보를앞으로전달합니다.

다만, 실제 전류와달리 퍼셉 트론신호는 '흐른다/안흐른다(1이나0)'의 두가지값을가질수있습니다. 이책에서는1을 ' 신호가흐른다' ,0을' 신호가흐르지않는다' 라는의미로쓰겠습니다.

### Weight and Bias
weight = 노드와 노드간 연결 강도
bias = 모든 예측들의 오프셋, y축을 지나는 선의 절편
[[Logical Gates]] 참고

## 퍼셉트론의 한계
### XOR 게이트 → 배타적 논리합
지금까지본퍼셉트론으로는이XOR게이트를구현할수없습니다. 왜AND와OR는 되고XOR는안될까요? 그림으로그려가며시각적으로설명해보겠습니다.
우선 XOR게이트의동작을시각적으로생각해보죠.
![|275](https://i.imgur.com/ICE02CV.png)

이 를 극 복 하 는 방 안 으 로 입 력 층 과 출 력 층 사 이 에 하 나 이 상 의 중 간 층( 은 닉 증 ) 을 두 어 비 선 형 적 으 로분리되는데이터에대해서도학습이가능하도록다층퍼셉트론 (multi-layer perceptron)을고안했습니다.
### Step Function
```python
import numpy as np
import matplotlib.pylab as plt


def step_function(x):
    y = x > 0
    return y.astype(np.int64)

x = np.arange(-5.0, 5.0, 0.1)
y = step_function(x)
plt.plot(x, y)
plt.ylim(-0.1, 1.1)
plt.show()
```

![|625](https://i.imgur.com/5OzUsan.png)

### Sigmoid Function
$$ h(x) = \frac{1}{1 + exp(-x)} $$
```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

x = np.arange(-5.0, 5.0, 0.1)
y = sigmoid(x)
plt.plot(x, y)
plt.ylim(-0.1, 1.1)
plt.show()
```

![|625](https://i.imgur.com/BfyYEDX.png)


### Sigmoid vs Step Function
가장먼저느껴지는점은'매끄러움'의차이일것입니다. 

시그모이드함수 는부드러운곡선이며입력에따라출력이연속적으로변화합니다. 
한편, 계단함수는0을경 계로출력이갑자기바뀌어버립니다. 시그모이드함수의이매끈함이신경망학습에서아주중 요한역할을하게됩니다.

(역시매끈함과관련되지만) 계단함수가0과1 중하나의값만돌려주는 반면
시그모이드함수는실수(0. 731..., 0.880.. 등) 를돌려준다는점도다릅니다. 

<mark style="background: #C6AB16;">다시말해 퍼셉트론에서는 뉴런 사이에0 혹은1이흘렀다면, 신경망에서는 연속적인실수가흐릅니다.</mark>
비유하자면, 계단함수는'시시오도시'*이고시그모이드함수는'물레방아'와비슷하죠. 
계단함수는시시오도시처럼물을쏟아내거나쏟아내지않는(0 또는1) 두가지움직임을보여주며, 
시그모이드함수는물레방아처럼흘러온물의양에비례해흐르는물의양을조절합니다.

#### 비선형 함수 (non-linear)
신경망에서는 활성화 함수로 비선형함수를 사용해야합니다. 
달리말하면선형함수를사용해서는안됩니다. 

**왜선형함수는안되는걸까요?** 
**→ 그이유는 바로 선형함수를이용하면 신경망의 층을 깊게하는 의미가 없어지기 때문입니다.**

## Perceptron vs Neural Network
### 퍼셉트론의 한계
- 퍼셉트론으로 복잡한 함수도 표현할 수 있다.
- 컴퓨터가 수행하는 복잡한 처리도 퍼셉트론으로 표현할수 있다.
- 하지만 가중치를 설정하는 작업은 여전히 사람이 수동으로 해야한다.
- AND OR 게이트 표를 보면서 인간이 적절한 ‘가중치’ 값을 조절해야했었다.
### 신경망으로 
- 신경망은 가중치 매개변수의 적절한 값을 데이터로 부터 자동으로 학습하는 성질을 가지고있다.
- 그래서 우리는 신경망이 입력 데이터를 식별하고 어떻게 처리하는지 과정을 아는것이 중요하다.
![|750](https://i.imgur.com/g8QVaDr.png)

#### 신경망 구현
- W: 가중치
- b: 편향
```python
def init_network():
	network = {}
	network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])
	network['b1'] = np.array([[0.1, 0.2, 0.3]])
	
	network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])
	network['b2'] = np. array([0.1, 0.2])
	
	network['W3'] = np. array([[0.1, 0.3], [0.2, 0.4]]) 
	network['b3'] = np. array([0.1, 0.2])
	return network

def forward(network, x):
	W1, W2, W3 = network['W1'], network['W2'], network['W3'] 
	b1, b2, b3 = network['b1'], network['b2'], network['b3']
	a1 = np. dot(x, W1) + b1 z1 = sigmoid(a1)
	a2 =np.dot(z1, W2) +b2 2 = sigmoid(a2)
	a3 = np. dot(z2, W3) + b3 
	y = identity_function(a3)
	return y

network = init_network()
× = np.array([1.0, 0.5])
y = forward(network, x)
print(y) # [ 0.31682708 0.69627909]
```


### 은닉층
인간의 뇌에 있는 수많은 뉴런들은 인간의 의지에 따라서 신호를 받아 결과를 출력합니다.

그래서 인간은 자신의 신체를 자유롭게 움직일 수 있는 것이죠.

신경망은 이러한 뉴런 시스템을 수학적으로 접근하여 개발되었습니다.

'은닉층(hidden layer)'은 신경망에서 더욱 복잡한 연산을 위해 가상의 뉴런을 생성하는 것입니다.

은닉층이 많을수록 신경망은 더욱 어려운 예측에 성공할 수 있습니다.

은닉층이 많은, 즉 2개 이상인 신경망을 딥러닝(deep learning)이라고 합니다!

동전의 앞면 뒷면을 맞추는 것과 강아지의 품종을 맞추는 것 중 어떤 일이 더 어려울까요?

동전의 앞면 뒷면을 맞추는 것은 확실한 특징이 있지만, 강아지의 품종은 종류도 워낙 많고 비슷한 사례가 있어서 어려울 수 있습니다.

딥러닝은 강아지의 품종을 예측하는 것과 같은 어려운 문제에서 강력한 성능을 보입니다.

### 출력층
신경망은 분류와 회귀 모두에 이용할 수 있습니다. 다만 둘중 어떤문제냐에 따라출력층에서 사용하는 활성화 함수가 달라집니다. 
일반적으로 **회귀에는 항등함수**
**분류에는 소프트맥스 함수를사용합니다**

#### 활성화 함수
활성 화 함수는 임계값을 경계로 출력이 바뀌는데, 이런 함수를 계단 함수라고 한다. 그래서 퍼셉트론에서는 활성화 함수로 계단 함수를 이용하고 신경망에서는 주로 시그모이드 함수를 사용한다.

#### 항등 함수 (identity function)
항등함수idenitlyhunction는입력을그대로출력합니다. 
입력과 출력이항상같다는 뜻의 항등입니다
→ 출력층에서 항등 함수를 사용하면 입력 신호가 그대로 출력 신호가 된다.
![|256](https://i.imgur.com/UE4nYah.png)

```python
def identity_function(x):
	return x
	
W3 = np,array([0.1, 0.3], [0.2, 0.4])
B3 = np.array([0.1, 0.2])

A3 = np.dot(Z2, W3) + B3
Y = identity_function(A3)
```

> [!info] 활성화 함수
> 출력층의 활성화 함수는 풀고자 하는 문제의 성질에 맞게 정합니다. 예를 들어 회귀에는 항등 함수를
> 2클래스 분류에는 시그모이드 함수를, 다중 클래스 분류에는 소프트맥스 함수를 사용한느것이 일반적이다.


#### 소프트맥스 함수 (softmax) 중요!!
한편, 분류에서사용하는소프트맥스함수 soft max function의식은다음과같습니다.

$$ Y_{k} = \frac{exp(a_{k})}{\sum_{i=1}^nexp(a_{i})} $$
- exp(x) 는 e^x 를 뜻하는 exponential function (지수 함수) 입니다.
- n 은 출력층의 뉴런 수
- yk는 그중 k 번째 출력입을 뜻합니다. 
- 소프트 맥스함수의 **분자는 입력 신호 ak 의 지수함수**
- 소프트 맥스함수의 **분모는 모든 입력 신호의 지수함수의 합** 으로 구성됩니다. 
![|405](https://i.imgur.com/cguuiHE.png)

→ 그림과 같이 소프트맥스의 출력은 모든 입력 신호로부터 화살표를 받습니다. 분모에서 보듯, 출력층의 각 뉴런이 모든 입력 신호에서 영향을 받기 때문입니다.

#### 구현
```python
import numpy as np

a = np.array([0.3, 2.9, 4.0])
exp_a = np.exp(a)
print(exp_a)
>> [ 1.34985881 18.17414537 54.59815003]

sum_exp_a = np.sum(exp_a)
print(sum_exp_a)
>> 74.1221542101633

y = exp_a / sum_exp_a
print(y)
>> [0.01821127 0.24519181 0.73659691]


def softmax(a):
	c = np.max(a)
	exp_a = np.exp(a - c) # overflow 대비
	sum_exp_a = np.sum(exp_a)
	y = exp_a / sum_exp_a
	return y
```

#### 소프트 맥스의 중요한 특징
```python
a = np.array([0.3, 2.9, 4.0])
y = softmax(a)
print(y)
>> [0.01821127 0.24519181 0.73659691]
# y1 = 0.018 = 1.8%
# y2 = 0.245 = 0.25%
# y3 = 0.735 = 0.73%
  
np.sum(y)
>> 1.0
```
- 소프트맥스 함수의 출력은 0에서 1.0 사이의 실수입니다
- 소프트맥스 총합은 1이다.

이 성질 덕분에 소프트맥스 함수의 출력을 ‘확률’로 해석할수 있다.
가령 앞의 예에서 y\[0] 의 확룔은 약 1.8%, y\[1] 은 24.5, y\[2]는 73.7로 해석할수있습니다.
그리고 이 결과 확률들로부터 “2번째 원소의 확률이 가장 높으니 답은 2번째 클래스다”라고 할 수 있습니다.

즉, 소프트맥스 함수를 이용함으로써 문제를 확률적으로 대응할수 있게 되는것이죠.
여기서 주의점은, 소프트맥스 함수를 적용해도 지수 함수가 단조 증가 함수 ( y = exp(x) )이기 때문에 각 원소의 대소관계는 변하지 않는다. 

→ 신경망을 이용한 분류에서는 일반적으로 가장 큰 출력을 내는 뉴런에 해당하는 클래스로만 인식합니다. 그리고 소프트맥스 함수를 적용해도 출력이 가장 큰 뉴런의 위치는 달라지지 않습니다. 현업에서도 지수 함수 곗나에 드는 자원 낭비를 줄이고자 출력층의 소프트맥스 함수는 생략하는것이 일반적이다.

> [!note] 
> 기계 학습의 문제풀이는 학습 & 추론 의 두단계를 거쳐 이뤄집니다. 
> 학습 단계에서 → 모델을 학습하고 (직업 훈련을 받고), 
> 추론 단계에서 → 앞서 학습한 모델로 미지의 데이터에 대해서 추론(분류)을 수행합니다 (현장 일 수행).
> 
> 기계학습에선 추론 단계에서는 출력층의 소프트맥스 함수를 생략하는것이 일반적입니다.
> 한편, 신경망을 학습 시킬때는 출력층에서 소프트맥스 함수를 사용합니다.


#### 출력층의 뉴런 수 정하기
메인 포인트: 출력층의 뉴런 수는 풀려는 문제에 맞게 적절히 정해야한다. 
→ 분류에서는 분류하고 싶은 클래스 수로 설정한느것이 일반적.

예시) 이미지 숫자 0부터 9중 하나로 분류하는 문제라면 아래처럼 출력층의 뉴런을 10개로 설정한다.
![|700](https://i.imgur.com/l2I9GV9.png)

출력층 뉴런은 위에서부터 차례로 숫자 0,1, … 9에 대응하며, 뉴런의 회색 농도가 해당 뉴런의 출력 값의 크기를 의미합니다. 이 예에서는 색이 가장 짙은 y2 뉴런이 가장 큰 값을 출력하는것이죠. 그래서 이 신경망이 선택한 클래스는 y2, 즉 입력 이미지를 숫자 ‘2’로 판단했음을 의미한다.