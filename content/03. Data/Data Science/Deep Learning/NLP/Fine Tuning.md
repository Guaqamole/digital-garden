---
title: Fine Tuning
date: 2024-05-09
draft: false
tags: 
complete: true
---
## Fine Tuning Process
### 1. Prepare Data
- download data
### 2. Process Data
- Use tokenizer to convert text to numbers for model
- Convert label to number if necessary
- Padding & Truncating
- Dynamic Padding (if necessary)
### 3. Training
### 4. Evaluating

PyTorchì˜ í•œ ë°°ì¹˜ì— ëŒ€í•´ Sequence Classifier ë¥¼ í›ˆë ¨í•˜ëŠ” ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

```python
import torch
from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification

# Same as before
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")

# This is new
batch["labels"] = torch.tensor([1, 1])

optimizer = AdamW(model.parameters())
loss = model(**batch).loss
loss.backward()
optimizer.step()
```

ë¬¼ë¡  ë‘ ë¬¸ì¥ìœ¼ë¡œ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ëŠ” ê²ƒë§Œìœ¼ë¡œëŠ” ì¢‹ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ì–»ìœ¼ë ¤ë©´ ë” í° ë°ì´í„°ì„¸íŠ¸ë¥¼ ì¤€ë¹„í•´ì•¼ í•©ë‹ˆë‹¤.

## Load Data
í—ˆê¹…í˜ì´ìŠ¤ ë°ì´í„° í—ˆë¸Œì—ëŠ” ëª¨ë¸ë§Œ í¬í•¨ë˜ëŠ” ê²ƒì´ ì•„ë‹™ë‹ˆë‹¤. ë‹¤ì–‘í•œ ì–¸ì–´ë¡œ ëœ ì—¬ëŸ¬ ë°ì´í„° ì„¸íŠ¸ê°€ ìˆìŠµë‹ˆë‹¤.Â [ì—¬ê¸°ì—ì„œ](https://huggingface.co/datasets)Â ë°ì´í„°ì„¸íŠ¸ë¥¼ ì°¾ì•„ë³¼ ìˆ˜ ìˆìœ¼ë©°Â , ì´ ì„¹ì…˜ì„ ì™„ë£Œí•œ í›„ ìƒˆ ë°ì´í„°ì„¸íŠ¸ë¥¼ ë¡œë“œí•˜ê³  ì²˜ë¦¬í•´ ë³´ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤(Â [ì—¬ê¸°ì—ì„œ](https://huggingface.co/docs/datasets/loading)Â ì¼ë°˜ ë¬¸ì„œ ì°¸ì¡° ). í•˜ì§€ë§Œ ì§€ê¸ˆì€ MRPC ë°ì´í„° ì„¸íŠ¸ì— ì§‘ì¤‘í•˜ê² ìŠµë‹ˆë‹¤! 

**ì´ëŠ”Â 10ê°œì˜ ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ ë¶„ë¥˜ ì‘ì—…ì— ê±¸ì³ ML ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” í•™ë¬¸ì  ë²¤ì¹˜ë§ˆí¬ì¸Â [GLUE ë²¤ì¹˜ë§ˆí¬ë¥¼](https://gluebenchmark.com/)Â êµ¬ì„±í•˜ëŠ” 10ê°œì˜ ë°ì´í„° ì„¸íŠ¸ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤ .**

ğŸ¤— ë°ì´í„° ì„¸íŠ¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” í—ˆë¸Œì—ì„œ ë°ì´í„° ì„¸íŠ¸ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ìºì‹œí•˜ëŠ” ë§¤ìš° ê°„ë‹¨í•œ ëª…ë ¹ì„ ì œê³µí•©ë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì´ MRPC ë°ì´í„°ì„¸íŠ¸ë¥¼ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
raw_datasets
```

```python
DatasetDict({
    train: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 408
    })
    test: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 1725
    })
})
```

ë³´ì‹œë‹¤ì‹œí”¼Â `DatasetDict`í›ˆë ¨ ì„¸íŠ¸, ê²€ì¦ ì„¸íŠ¸ ë° í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ê°€ í¬í•¨ëœ ê°ì²´ë¥¼ ì–»ìŠµë‹ˆë‹¤. ê°ê°ì—ëŠ” ì—¬ëŸ¬ ì—´(Â `sentence1`,Â `sentence2`,Â `label`ë°Â `idx`)ê³¼ ê° ì„¸íŠ¸ì˜ ìš”ì†Œ ìˆ˜ì¸ ê°€ë³€ ê°œìˆ˜ì˜ í–‰ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤(ë”°ë¼ì„œ í›ˆë ¨ ì„¸íŠ¸ì—ëŠ” 3,668ê°œì˜ ë¬¸ì¥ ìŒ, ê²€ì¦ ì„¸íŠ¸ì—ëŠ” 408ê°œ, ê²€ì¦ ì„¸íŠ¸ì—ëŠ” 1,725ê°œì˜ ë¬¸ì¥ ìŒì´ ìˆìŠµë‹ˆë‹¤). í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ).

ì´ ëª…ë ¹ì€ ê¸°ë³¸ì ìœ¼ë¡œÂ _~/.cache/huggingface/datasets_Â ì— ë°ì´í„°ì„¸íŠ¸ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ìºì‹œí•©ë‹ˆë‹¤ .Â `HF_HOME`í™˜ê²½ ë³€ìˆ˜ë¥¼Â ì„¤ì •í•˜ì—¬ ìºì‹œ í´ë”ë¥¼ ì‚¬ìš©ì ì •ì˜í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ 2ì¥ì—ì„œ ìƒê¸°í•´ ë³´ì„¸ìš” .

`raw_datasets`ì‚¬ì „ì²˜ëŸ¼ ìƒ‰ì¸ì„ ìƒì„±í•˜ì—¬ ê°ì²´Â ì˜ ê° ë¬¸ì¥ ìŒì— ì•¡ì„¸ìŠ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ .
```python
raw_train_dataset = raw_datasets["train"]
raw_train_dataset[0]
```

```python
{'idx': 0,
 'label': 1,
 'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
 'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .'}
```

ë ˆì´ë¸”ì€ ì´ë¯¸ ì •ìˆ˜ì´ë¯€ë¡œ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤. ì–´ë–¤ ì •ìˆ˜ê°€ ì–´ë–¤ ë¼ë²¨ì— í•´ë‹¹í•˜ëŠ”ì§€ ì•Œê¸° ìœ„í•´ `raw_train_dataset` ì˜ `features`  ë¥¼ í™•ì¸í•  ìˆ˜Â ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ê° ì—´ì˜ feature ì„ ì•Œë ¤ì¤ë‹ˆë‹¤.
```python
raw_train_dataset.features
```

```python
{'sentence1': Value(dtype='string', id=None),
 'sentence2': Value(dtype='string', id=None),
 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),
 'idx': Value(dtype='int32', id=None)}
```

Behind the scenes, `label` ì€Â `ClassLabel` íƒ€ì…ì´ë©°Â ì •ìˆ˜ì™€ ë ˆì´ë¸” nameì˜ ë§¤í•‘ì€Â _names_Â í´ë”ì— ì €ì¥ë©ë‹ˆë‹¤.Â `0`ì€Â `not_equivalent` ì„ ì˜ë¯¸í•˜ê³  `1`ì€ `equivalent` ì…ë‹ˆë‹¤.

## Preprocess Data
ë°ì´í„°ì„¸íŠ¸ë¥¼ ì „ì²˜ë¦¬í•˜ë ¤ë©´ í…ìŠ¤íŠ¸ë¥¼ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ìˆ«ìë¡œ ë³€í™˜í•´ì•¼ í•©ë‹ˆë‹¤.Â [ì´ì „ ì¥](https://huggingface.co/course/chapter2)Â ì—ì„œ ë³¸ ê²ƒì²˜ëŸ¼Â ì´ëŠ” í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ ìˆ˜í–‰ë©ë‹ˆë‹¤. í† í¬ë‚˜ì´ì €ì— í•œ ë¬¸ì¥ ë˜ëŠ” ë¬¸ì¥ ëª©ë¡ì„ ì œê³µí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë‹¤ìŒê³¼ ê°™ì´ ê° ìŒì˜ ëª¨ë“  ì²« ë²ˆì§¸ ë¬¸ì¥ê³¼ ëª¨ë“  ë‘ ë²ˆì§¸ ë¬¸ì¥ì„ ì§ì ‘ í† í°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
```python
from transformers import AutoTokenizer

checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenized_sentences_1 = tokenizer(raw_datasets["train"]["sentence1"])
tokenized_sentences_2 = tokenizer(raw_datasets["train"]["sentence2"])
```

ê·¸ëŸ¬ë‚˜ ë‹¨ì§€ ë‘ ì‹œí€€ìŠ¤ë§Œ ëª¨ë¸ì— ì „ë‹¬í•˜ê³  ë‘ ë¬¸ì¥ì´ passphrase ì¸ì§€ ì•„ë‹Œì§€ì— ëŒ€í•´ ì˜ˆì¸¡ í• ìˆ˜ëŠ” ì—†ìŠµë‹ˆë‹¤. ë‘ ì‹œí€€ìŠ¤ë¥¼ í•œ ìŒìœ¼ë¡œ ì²˜ë¦¬í•˜ê³  ì ì ˆí•œ ì „ì²˜ë¦¬ë¥¼ ì ìš©í•´ì•¼ í•©ë‹ˆë‹¤. ë‹¤í–‰íˆë„ í† í¬ë‚˜ì´ì €ëŠ” í•œ ìŒì˜ ì‹œí€€ìŠ¤ë¥¼ ê°€ì ¸ì™€ BERT ëª¨ë¸ì´ ì˜ˆìƒí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì¤€ë¹„í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

```python
inputs = tokenizer("This is the first sentence.", "This is the second one.")
inputs
```

```python
{ 
  'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102],
  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
}
```

[2ì¥](https://huggingface.co/course/chapter2)Â ì—ì„œÂ `input_ids`ë° `attention_mask` í‚¤ì—Â ëŒ€í•´ ë…¼ì˜í–ˆì§€ë§ŒÂ `token_type_ids` ì— ëŒ€í•œ ì´ì•¼ê¸°ëŠ” ë’¤ë¡œ ë¯¸ë£¨ì—ˆìŠµë‹ˆë‹¤Â . ì´ ì˜ˆì—ì„œëŠ” ì…ë ¥ì˜ ì–´ëŠ ë¶€ë¶„ì´ ì²« ë²ˆì§¸ ë¬¸ì¥ì´ê³  ì–´ëŠ ë¶€ë¶„ì´ ë‘ ë²ˆì§¸ ë¬¸ì¥ì¸ì§€ ëª¨ë¸ì— ì•Œë ¤ì¤ë‹ˆë‹¤.

`input_ids`ë‚´ë¶€ì˜ IDë¥¼ ë‹¤ì‹œ ë‹¨ì–´ë¡œÂ ë””ì½”ë”©í•˜ë©´ ë‹¤ìŒ ê³¼ ê°™ìŠµë‹ˆë‹¤.

```python
tokenizer.convert_ids_to_tokens(inputs["input_ids"])
```

ê·¸ í›„ ì´ë ‡ê²Œ ì¶œë ¥ë©ë‹ˆë‹¤.
```python
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
```

`[CLS] sentence1 [SEP] sentence2 [SEP]`ë”°ë¼ì„œ ëª¨ë¸ì€ ì…ë ¥ì´ ë‘ ê°œì˜ ë¬¸ì¥ì´ ìˆì„ ë•ŒÂ ì˜ í˜•ì‹ì´ ë  ê²ƒìœ¼ë¡œ ì˜ˆìƒí•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤ .Â ì´ë¥¼Â `token_type_ids`ë‹¤ìŒê³¼ ê°™ì´ ì¡°ì •í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ì´ì ì´ ìˆìŠµë‹ˆë‹¤.

```python
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
[      0,      0,    0,     0,       0,          0,   0,       0,      1,    1,     1,        1,     1,   1,       1]
```

ë³´ì‹œë‹¤ì‹œí”¼,Â `[CLS] sentence1 [SEP]`ëª¨ë‘ì— í•´ë‹¹í•˜ëŠ” ì…ë ¥ ë¶€ë¶„ì˜ í† í° ìœ í˜• IDëŠ” ì´ê³ Â `0`, ì— í•´ë‹¹í•˜ëŠ” ë‹¤ë¥¸ ë¶€ë¶„ì˜Â `sentence2 [SEP]`í† í° ìœ í˜• IDëŠ” ëª¨ë‘ ì…ë‹ˆë‹¤Â `1`.

ë‹¤ë¥¸ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì„ íƒí•˜ëŠ” ê²½ìš° í† í°í™”ëœ ì…ë ¥ì— ë°˜ë“œì‹œ `token_type_ids`ê°€ ìˆì„ í•„ìš”ëŠ” ì—†ìŠµë‹ˆë‹¤Â (ì˜ˆë¥¼ ë“¤ì–´ DistilBERT ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ë°˜í™˜ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤). ëª¨ë¸ì´ ì‚¬ì „ í›ˆë ¨ ì¤‘ì— ì´ë¥¼ ë³´ì•˜ê¸° ë•Œë¬¸ì— ëª¨ë¸ì´ ì´ë¥¼ ì–´ë–»ê²Œ í•´ì•¼ í• ì§€ ì•Œê²Œ ë  ë•Œë§Œ ë°˜í™˜ë©ë‹ˆë‹¤.

[ì—¬ê¸°ì„œ BERTëŠ” í† í° ìœ í˜• IDë¡œ ì‚¬ì „ í•™ìŠµë˜ì—ˆìœ¼ë©°, 1ì¥](https://huggingface.co/course/chapter1)Â ì—ì„œ ì„¤ëª…í•œ ë§ˆìŠ¤í¬ëœ ì–¸ì–´ ëª¨ë¸ë§ ëª©í‘œ ì™¸ì—Â _ë‹¤ìŒ ë¬¸ì¥ ì˜ˆì¸¡_Â ì´ë¼ëŠ” ì¶”ê°€ ëª©í‘œê°€ ìˆìŠµë‹ˆë‹¤Â . ì´ ì‘ì—…ì˜ ëª©í‘œëŠ” ë¬¸ì¥ ìŒ ê°„ì˜ ê´€ê³„ë¥¼ ëª¨ë¸ë§í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ë‹¤ìŒ ë¬¸ì¥ ì˜ˆì¸¡ì„ í†µí•´ ëª¨ë¸ì—ëŠ” ë¬¸ì¥ ìŒ(ë¬´ì‘ìœ„ë¡œ ë§ˆìŠ¤í‚¹ëœ í† í° í¬í•¨)ì´ ì œê³µë˜ê³  ë‘ ë²ˆì§¸ ë¬¸ì¥ì´ ì²« ë²ˆì§¸ ë¬¸ì¥ ë‹¤ìŒì— ì˜¤ëŠ”ì§€ ì—¬ë¶€ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ ìš”ì²­ë©ë‹ˆë‹¤. ì‘ì—…ì„ ê°„ë‹¨í•˜ê²Œ ë§Œë“¤ê¸° ìœ„í•´ ì ˆë°˜ì˜ ì‹œê°„ ë™ì•ˆ ë¬¸ì¥ì€ ì¶”ì¶œëœ ì›ë³¸ ë¬¸ì„œì—ì„œ ì„œë¡œ ë”°ë¥´ê³  ë‚˜ë¨¸ì§€ ì ˆë°˜ì˜ ì‹œê°„ì€ ë‘ ë¬¸ì¥ì´ ë‘ ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ë¬¸ì„œì—ì„œ ë‚˜ì˜µë‹ˆë‹¤.

ì¼ë°˜ì ìœ¼ë¡œ í† í°í™”ëœ ì…ë ¥ì— `token_type_ids` ê°€ ìˆëŠ”ì§€ ì—¬ë¶€ì— ëŒ€í•´ ê±±ì •í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤. í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ì— ë™ì¼í•œ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ëŠ” í•œ í† í¬ë‚˜ì´ì €ëŠ” ëª¨ë¸ì— ë¬´ì—‡ì„ ì œê³µí• ì§€ ì•Œê³  ìˆìœ¼ë¯€ë¡œ ëª¨ë“  ê²ƒì´ ê´œì°®ì„ ê²ƒì…ë‹ˆë‹¤. .

ì´ì œ í† í¬ë‚˜ì´ì €ê°€ í•œ ìŒì˜ ë¬¸ì¥ì„ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ì„ ì‚´í´ë³´ì•˜ìœ¼ë¯€ë¡œ ì´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ì²´ ë°ì´í„° ì„¸íŠ¸ë¥¼ í† í°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Â [ì´ì „ ì¥](https://huggingface.co/course/chapter2)Â ì—ì„œì™€ ê°™ì´ ì²« ë²ˆì§¸ ë¬¸ì¥ ëª©ë¡ì„ ì œê³µí•˜ì—¬ í† í¬ë‚˜ì´ì €ì— ë¬¸ì¥ ìŒ ëª©ë¡ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¬¸ì¥, ê·¸ ë‹¤ìŒ ë‘ ë²ˆì§¸ ë¬¸ì¥ì˜ ëª©ë¡ì…ë‹ˆë‹¤. ì´ëŠ”Â [2ì¥](https://huggingface.co/course/chapter2)Â ì—ì„œ ë³¸ íŒ¨ë”© ë° ìë¥´ê¸° ì˜µì…˜ê³¼ë„ í˜¸í™˜ë©ë‹ˆë‹¤Â . ë”°ë¼ì„œ í•™ìŠµ ë°ì´í„° ì„¸íŠ¸ë¥¼ ì „ì²˜ë¦¬í•˜ëŠ” í•œ ê°€ì§€ ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

```python
tokenized_dataset = tokenizer(
    raw_datasets["train"]["sentence1"],
    raw_datasets["train"]["sentence2"],
    padding=True,
    truncation=True,
)
```

ì´ê²ƒì€ ì˜ ì‘ë™í•˜ì§€ë§Œ ì‚¬ì „(í‚¤, input_ids, attention_mask, token_type_ids ë° ëª©ë¡ì¸ ê°’)ì„ ë°˜í™˜í•œë‹¤ëŠ” ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤.

ë˜í•œ í† í°í™” ì¤‘ì— ë°ì´í„°ì…‹ ì „ì²´ë¥¼ ì €ì¥í•  ìˆ˜ ìˆëŠ” ì¶©ë¶„í•œ RAMì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ì‘ë™í•©ë‹ˆë‹¤(whereas ğŸ¤— ë°ì´í„°ì…‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ë°ì´í„°ì…‹ì€ ë””ìŠ¤í¬ì— ì €ì¥ëœ Apache Arrow íŒŒì¼ì´ë¯€ë¡œ ìš”ì²­í•œ ìƒ˜í”Œë§Œ ë©”ëª¨ë¦¬ì— ë¡œë“œí•©ë‹ˆë‹¤).

### Dataset.map()

ë°ì´í„°ë¥¼ ë°ì´í„°ì„¸íŠ¸ë¡œ ìœ ì§€í•˜ê¸° ìœ„í•´ ì´ `Dataset.map()` ë°©ë²•ì„ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤. ì´ëŠ” ë˜í•œ í† í°í™”ë³´ë‹¤ ë” ë§ì€ ì „ì²˜ë¦¬ê°€ í•„ìš”í•œ ê²½ìš° ì¶”ê°€ì ì¸ ìœ ì—°ì„±ì„ ì œê³µí•©ë‹ˆë‹¤. ì´Â `map()`ë°©ë²•ì€ ë°ì´í„° ì„¸íŠ¸ì˜ ê° ìš”ì†Œì— í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì—¬ ì‘ë™í•˜ë¯€ë¡œ ì…ë ¥ì„ í† í°í™”í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì •ì˜í•´ ë³´ê² ìŠµë‹ˆë‹¤.

```python
def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)
```

**ì´ í•¨ìˆ˜ëŠ” (ë°ì´í„° ì„¸íŠ¸ì˜ í•­ëª©ê³¼ ê°™ì€) ì‚¬ì „ì„ ê°€ì ¸ê°€ì„œ input_ids, attention_mask ë° token_type_ids í‚¤ë¡œ ìƒˆ ì‚¬ì „ì„ ë°˜í™˜í•©ë‹ˆë‹¤.** ì•ì—ì„œ ë³¸ ê²ƒì²˜ëŸ¼ í† í°í™”ê¸°ëŠ” ë¬¸ì¥ ìŒì˜ ëª©ë¡ì—ì„œ ì‘ë™í•˜ë¯€ë¡œ ì˜ˆì œ ì‚¬ì „ì— ì—¬ëŸ¬ ìƒ˜í”Œ(ê° í‚¤ëŠ” ë¬¸ì¥ ëª©ë¡ìœ¼ë¡œ)ì´ í¬í•¨ëœ ê²½ìš°ì—ë„ ì‘ë™í•©ë‹ˆë‹¤. **ì´ë ‡ê²Œ í•˜ë©´ batched=True ì˜µì…˜ì„ ì‚¬ìš©í•˜ì—¬ (),ë¥¼ ë§¤í•‘í•˜ì—¬ í† í°í™” ì†ë„ë¥¼ í¬ê²Œ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.** í† í°í™”ê¸°ëŠ” ğŸ¤— í† í°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ Rustë¡œ ì‘ì„±ëœ í† í°í™”ê¸°ì— ì˜í•´ ì§€ì›ë©ë‹ˆë‹¤. ì´ í† í°í™”ê¸°ëŠ” ë§¤ìš° ë¹ ë¥¼ ìˆ˜ ìˆì§€ë§Œ í•œ ë²ˆì— ë§ì€ ì…ë ¥ì„ í•´ì•¼ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤.  
  
í† í°í™” í•¨ìˆ˜ì—ì„œ íŒ¨ë”© ì¸ìˆ˜ëŠ” ì¼ë‹¨ ìƒëµí–ˆìŠµë‹ˆë‹¤. ì´ê²ƒì€ ëª¨ë“  ìƒ˜í”Œì„ ìµœëŒ€ ê¸¸ì´ë¡œ íŒ¨ë”©í•˜ëŠ” ê²ƒì´ íš¨ìœ¨ì ì´ì§€ ì•Šê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë©´ ì „ì²´ ë°ì´í„° ì„¸íŠ¸ì˜ ìµœëŒ€ ê¸¸ì´ê°€ ì•„ë‹ˆë¼ í•´ë‹¹ ë°°ì¹˜ì˜ ìµœëŒ€ ê¸¸ì´ë¡œë§Œ íŒ¨ë”©í•˜ë©´ ë˜ê¸° ë•Œë¬¸ì— ë°°ì¹˜ë¥¼ êµ¬ì¶•í•  ë•Œ ìƒ˜í”Œì„ íŒ¨ë”©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. ì´ëŠ” ì…ë ¥ì´ ë§¤ìš° ê°€ë³€ì ì¸ ê¸¸ì´ë¥¼ ê°€ì§ˆ ë•Œ ë§ì€ ì‹œê°„ê³¼ ì²˜ë¦¬ ëŠ¥ë ¥ì„ ì ˆì•½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!  
  
ë‹¤ìŒì€ í† í°í™” ê¸°ëŠ¥ì„ ëª¨ë“  ë°ì´í„° ì„¸íŠ¸ì— í•œ ë²ˆì— ì ìš©í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. í•¨ìˆ˜ê°€ ê° ìš”ì†Œì— ê°œë³„ì ìœ¼ë¡œ ì ìš©ë˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ë°ì´í„° ì„¸íŠ¸ì˜ ì—¬ëŸ¬ ìš”ì†Œì— í•œ ë²ˆì— ì ìš©ë˜ë„ë¡ ë§¤í•‘í•˜ê¸° ìœ„í•´ batched=Trueë¥¼ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë” ë¹ ë¥¸ ì „ì²˜ë¦¬ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.

```python
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
tokenized_datasets
```

ğŸ¤— ë°ì´í„° ì„¸íŠ¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì´ ì²˜ë¦¬ë¥¼ ì ìš©í•˜ëŠ” ë°©ë²•ì€ ì „ì²˜ë¦¬ í•¨ìˆ˜ì—ì„œ ë°˜í™˜ëœ ì‚¬ì „ì˜ ê° í‚¤ì— ëŒ€í•´ í•˜ë‚˜ì”© ë°ì´í„° ì„¸íŠ¸ì— ìƒˆ í•„ë“œë¥¼ ì¶”ê°€í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

```python
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 408
    })
    test: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 1725
    })
})
```

`map()`ìœ¼ë¡œ ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ì ìš©í•  ë•Œ num_ proc ì¸ìˆ˜ë¥¼ ì „ë‹¬í•˜ì—¬ ë©€í‹°í”„ë¡œì„¸ì‹±ì„ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ğŸ¤— Tokenizers ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ì´ë¯¸ ì—¬ëŸ¬ ìŠ¤ë ˆë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒ˜í”Œì„ ë” ë¹ ë¥´ê²Œ í† í°í™”í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì´ ì‘ì—…ì„ ìˆ˜í–‰í•˜ì§€ ì•Šì•˜ì§€ë§Œ, ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ ì§€ì›ë˜ëŠ” ë¹ ë¥¸ í† í°í™”ê¸°ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë©´ ì „ì²˜ë¦¬ ì†ë„ê°€ ë¹¨ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
  
tokenize_functionì€ í‚¤ input_ids, attention_mask ë° token_type_idsê°€ í¬í•¨ëœ ì‚¬ì „ì„ ë°˜í™˜í•˜ë¯€ë¡œ ì´ ì„¸ í•„ë“œëŠ” ë°ì´í„° ì„¸íŠ¸ì˜ ëª¨ë“  ë¶„í• ì— ì¶”ê°€ë©ë‹ˆë‹¤. ì „ì²˜ë¦¬ í•¨ìˆ˜ê°€ map()ì„ ì ìš©í•œ ë°ì´í„° ì„¸íŠ¸ì˜ ê¸°ì¡´ í‚¤ì— ëŒ€í•œ ìƒˆ ê°’ì„ ë°˜í™˜í•˜ë©´ ê¸°ì¡´ í•„ë“œë„ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
  
ë§ˆì§€ë§‰ìœ¼ë¡œ í•´ì•¼ í•  ì¼ì€ ìš”ì†Œë¥¼ í•¨ê»˜ ë°°ì¹˜í•  ë•Œ ê°€ì¥ ê¸´ ìš”ì†Œì˜ ê¸¸ì´ë¡œ ëª¨ë“  ì˜ˆì œë¥¼ íŒ¨ë”©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ ê¸°ë²•ì„ `dynamic padding`ì´ë¼ê³  í•©ë‹ˆë‹¤.

### dynamic padding
ë°°ì¹˜ ë‚´ë¶€ì˜ ìƒ˜í”Œì„ í•¨ê»˜ ë°°ì¹˜í•˜ëŠ”ë° ì±…ì„ì´ ìˆëŠ” ê¸°ëŠ¥ì€ `collate function` ì´ë¼ê³  ë¶ˆë¦°ë‹¤.Data Loaderë¥¼ êµ¬ì¶•í•˜ë©´ ìƒ˜í”Œ í…íŠ¸ í…ë”ë¡œ ë³€í™˜í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì´ë©°, ê¸°ë³¸ê°’ì€ poptors(Res)ë¥¼ êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ìš°ë¦¬ê°€ ê°™ì€ í¬ê¸°ì˜ ì…ë ¥ì´ ì—†ëŠ” ê²½ìš°, ìš°ë¦¬ê°€ í•  ìˆ˜ ì—†ì„ ê²ƒì…ë‹ˆë‹¤.ìš°ë¦¬ëŠ” íŒ¨ë”©ì— í•„ìš”í•œ íŒ¨ë”©ìœ¼ë¡œ ì¸í•´ íŒ¨ë”©ì— í•„ìš”í•œ ë§Œí¼ íŒ¨ë”©ìœ¼ë¡œ ì¸í•´ íŒ¨ë”©ì´ ë§ì´ ì—°ê¸°ë˜ì—ˆë‹¤.ì´ í›ˆë ¨ ì†ë„ë¥¼ ìƒë‹¹íˆ ë¹ ë¥´ê²Œ ì§„í–‰í•˜ì§€ë§Œ TPUëŠ” TPUê°€ í•„ìš”í•œ ê²½ìš°, TPUëŠ” ê³ ì • í˜•ìƒì„ ì¼ìœ¼í‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
  
ì´ ì‘ì—…ì„ ìˆ˜í–‰í•˜ë ¤ë©´, ìš°ë¦¬ëŠ” í•¨ê»˜ ë°°ì¹˜í•˜ë ¤ëŠ” ë°ì´í„° ì„¸íŠ¸ì˜ ì˜¬ë°”ë¥¸ íŒ¨ë”© ê¸°ëŠ¥ì„ ì •ì˜í•´ì•¼ í•©ë‹ˆë‹¤.ë‹¤í–‰íˆë„ ğŸ¤— transformer ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” `DataCollatorWithPadding` ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. ì¸ìŠ¤í„´ìŠ¤í™” í• ë•Œ ( í˜¹ì€ ì‚¬ìš©í•  íŒ¨ë”© í† í°, ëª¨ë¸ì´ ì…ë ¥ì˜ ì™¼ìª½ ë˜ëŠ” ì˜¤ë¥¸ìª½ì— íŒ¨ë”©ì„ ì˜ˆìƒí•˜ëŠ”ì§€ ì—¬ë¶€ë¥¼ íŒŒì•…í•˜ê¸° ìœ„í•´) í† í¬ë‚˜ì´ì €ë¥¼ ì¸ìë¡œ ë°›ê³  ë‹¹ì‹ ì´ í•„ìš”í•œ ëª¨ë“ ê²ƒì„ ì œê³µí•©ë‹ˆë‹¤.
```python
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

ì´ ìƒˆë¡œìš´ ì¥ë‚œê°ì„ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•´ í•¨ê»˜ ë°°ì¹˜í•  í›ˆë ¨ ì„¸íŠ¸ì—ì„œ ëª‡ ê°€ì§€ ìƒ˜í”Œì„ ê°€ì ¸ì™€ ë³´ê² ìŠµë‹ˆë‹¤. ì—¬ê¸°ì—ì„œëŠ” `sentence1` , `sentence2`í•„ìš”í•˜ì§€Â ì•Šê³  ë¬¸ìì—´ì„ í¬í•¨í•˜ëŠ” ì—´(ë° ë¬¸ìì—´ë¡œ í…ì„œë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŒ)Â `idx`ì„Â ì œê±°í•˜ê³ Â ë°°ì¹˜ì˜ ê° í•­ëª© ê¸¸ì´ë¥¼ ì‚´í´ë´…ë‹ˆë‹¤.

```python
samples = tokenized_datasets["train"][:8]
samples = {k: v for k, v in samples.items() if k not in ["idx", "sentence1", "sentence2"]}
[len(x) for x in samples["input_ids"]]
```

```python
[50, 59, 47, 67, 59, 50, 62, 32]
```

ë‹¹ì—°íˆ 32ì—ì„œ 67ê¹Œì§€ ë‹¤ì–‘í•œ ê¸¸ì´ì˜ ìƒ˜í”Œì„ ì–»ìŠµë‹ˆë‹¤. ë™ì  íŒ¨ë”©ì€ ì´ ë°°ì¹˜ì˜ ìƒ˜í”Œì´ ëª¨ë‘ ë°°ì¹˜ ë‚´ë¶€ì˜ ìµœëŒ€ ê¸¸ì´ì¸ 67ì˜ ê¸¸ì´ë¡œ íŒ¨ë”©ë˜ì–´ì•¼ í•¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ë™ì  íŒ¨ë”©ì´ ì—†ìœ¼ë©´ ëª¨ë“  ìƒ˜í”Œì€ ì „ì²´ ë°ì´í„° ì„¸íŠ¸ì˜ ìµœëŒ€ ê¸¸ì´ ë˜ëŠ” ëª¨ë¸ì´ í—ˆìš©í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ ê¸¸ì´ê¹Œì§€ íŒ¨ë”©ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.Â `data_collator` ê°€ ì¼ê´„ ì²˜ë¦¬ê°€ ì ì ˆí•˜ê²Œ ë™ì ìœ¼ë¡œ ì±„ì›Œì§€ê³  ìˆëŠ”ì§€Â ë‹¤ì‹œ í™•ì¸í•´ ë³´ê² ìŠµë‹ˆë‹¤ .

```python
batch = data_collator(samples)
{k: v.shape for k, v in batch.items()}
```

```python
{'attention_mask': torch.Size([8, 67]),
 'input_ids': torch.Size([8, 67]),
 'token_type_ids': torch.Size([8, 67]),
 'labels': torch.Size([8])}
```

ë“œë””ì–´ ì´ì œ raw í…ìŠ¤íŠ¸ì—ì„œ ëª¨ë¸ì´ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ë°°ì¹˜ë¡œ ì „í™˜í–ˆìœ¼ë¯€ë¡œ fine-tuning í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤!

## Training
ğŸ¤— TransformersëŠ”Â `Trainer`ë°ì´í„°ì„¸íŠ¸ì—ì„œ ì œê³µí•˜ëŠ” ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” `Trainer`ë° ë„ì›€ì´ ë˜ëŠ” í´ë˜ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ë§ˆì§€ë§‰ ì„¹ì…˜ì—ì„œ ëª¨ë“  ë°ì´í„° ì „ì²˜ë¦¬ ì‘ì—…ì„ ì™„ë£Œí•˜ê³  ë‚˜ë©´ ê°€ì¥ ì–´ë ¤ìš´ ë¶€ë¶„Â `Trainer.train()`ì€ CPUì—ì„œ ë§¤ìš° ëŠë¦¬ê²Œ ì‹¤í–‰ë˜ë¯€ë¡œÂ ì‹¤í–‰í•  í™˜ê²½ì„ ì¤€ë¹„í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤ .Â GPUê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•Šì€ ê²½ìš°Â [Google Colab](https://colab.research.google.com/)Â ì—ì„œ ë¬´ë£Œ GPU ë˜ëŠ” TPUì— ì•¡ì„¸ìŠ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ .

ì•„ë˜ ì½”ë“œ ì˜ˆì œì—ì„œëŠ” ì´ì „ ì„¹ì…˜ì˜ ì˜ˆì œë¥¼ ì´ë¯¸ ì‹¤í–‰í–ˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤. ë‹¤ìŒì€ í•„ìš”í•œ ì‚¬í•­ì„ ê°„ëµí•˜ê²Œ ìš”ì•½í•œ ê²ƒì…ë‹ˆë‹¤.

```python
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

### Start Training
`Trainer` ë¥¼ ì •ì˜í•˜ê¸° ì „ ì²« ë²ˆì§¸ ë‹¨ê³„ëŠ”Â í›ˆë ¨ê³¼ í‰ê°€ì— ì‚¬ìš©í• Â ëª¨ë“  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ í¬í•¨í•˜ëŠ” í´ë˜ìŠ¤ì¸ `TrainingArguments`ë¥¼ ì •ì˜í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤ .Â ì œê³µí•´ì•¼ í•˜ëŠ” ìœ ì¼í•œ ì¸ìˆ˜ëŠ” í•™ìŠµëœ ëª¨ë¸ì´ ì €ì¥ë  ë””ë ‰í„°ë¦¬ì™€ ê·¸ ê³¼ì •ì—ì„œ ì²´í¬í¬ì¸íŠ¸ì…ë‹ˆë‹¤. ë‚˜ë¨¸ì§€ ëª¨ë“  ì‚¬í•­ì— ëŒ€í•´ì„œëŠ” ê¸°ë³¸ê°’ì„ ê·¸ëŒ€ë¡œ ë‘ë©´ ê¸°ë³¸ ë¯¸ì„¸ ì¡°ì •ì— ë§¤ìš° ì í•©í•©ë‹ˆë‹¤. 

```python
from transformers import TrainingArguments

training_args = TrainingArguments("test-trainer")
```

ë‘ ë²ˆì§¸ ë‹¨ê³„ëŠ” ëª¨ë¸ì„ ì •ì˜í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.Â [ì´ì „ ì¥](https://huggingface.co/course/chapter2)Â ì—ì„œì™€ ë§ˆì°¬ê°€ì§€ë¡œ ë‘ ê°œì˜ ë ˆì´ë¸”ì´ ìˆëŠ”`AutoModelForSequenceClassification` í´ë˜ìŠ¤ë¥¼Â ì‚¬ìš©í•©ë‹ˆë‹¤Â .

```python
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

[2ì¥](https://huggingface.co/course/chapter2)Â ê³¼ ë‹¬ë¦¬ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•œ í›„ ê²½ê³ ê°€ í‘œì‹œëœë‹¤ëŠ”Â ì ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤ .Â ì´ëŠ” BERTê°€ ë¬¸ì¥ ìŒì„ ë¶„ë¥˜í•˜ëŠ” ë° ì‚¬ì „ í•™ìŠµë˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì— ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì˜ í—¤ë“œë¥¼ ì‚­ì œí•˜ê³  ëŒ€ì‹  ì‹œí€€ìŠ¤ ë¶„ë¥˜ì— ì í•©í•œ ìƒˆë¡œìš´ í—¤ë“œë¥¼ ì¶”ê°€í–ˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ê²½ê³ ëŠ” ì¼ë¶€ ê°€ì¤‘ì¹˜(ì‚­ì œëœ ì‚¬ì „ í›ˆë ¨ í—¤ë“œì— í•´ë‹¹í•˜ëŠ” ê°€ì¤‘ì¹˜)ê°€ ì‚¬ìš©ë˜ì§€ ì•Šì•˜ìœ¼ë©° ì¼ë¶€ ê°€ì¤‘ì¹˜(ìƒˆ í—¤ë“œì— ëŒ€í•œ ê°€ì¤‘ì¹˜)ê°€ ë¬´ì‘ìœ„ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ìš°ë¦¬ê°€ ì§€ê¸ˆ í•˜ë ¤ê³  í•˜ëŠ” ê²ƒì´ ë°”ë¡œ ëª¨ë¸ í›ˆë ¨ì„ ê¶Œì¥í•˜ëŠ” ê²ƒìœ¼ë¡œ ë§ˆë¬´ë¦¬ë©ë‹ˆë‹¤.

ëª¨ë¸ì´ ìˆìœ¼ë©´Â `Trainer`ì§€ê¸ˆê¹Œì§€ êµ¬ì„±ëœ ëª¨ë“  ê°œì²´( , í›ˆë ¨ ë° ê²€ì¦ ë°ì´í„° ì„¸íŠ¸, our ë° our :)ë¥¼ ì „ë‹¬í•˜ì—¬Â `model`aÂ `training_args`ë¥¼Â `data_collator`ì •ì˜Â `tokenizer`í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
from transformers import Trainer

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
)
```

Note that when you pass theÂ `tokenizer`Â as we did here, the defaultÂ `data_collator`Â used by theÂ `Trainer`Â will be aÂ `DataCollatorWithPadding`Â as defined previously, so you can skip the lineÂ `data_collator=data_collator`Â in this call. It was still important to show you this part of the processing in section 2!

To fine-tune the model on our dataset, we just have to call theÂ `train()`Â method of ourÂ `Trainer`:

```python
trainer.train()
```

ê·¸ëŸ¬ë©´ fine-tuning (GPUì—ì„œëŠ” ëª‡ ë¶„ ì •ë„ ì†Œìš”)ì´ ì‹œì‘ë˜ê³  500ë‹¨ê³„ë§ˆë‹¤ í›ˆë ¨ ì†ì‹¤ì´ ë³´ê³ ë©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ì–¼ë§ˆë‚˜ ì¢‹ì€ì§€(ë˜ëŠ” ë‚˜ìœì§€) ì•Œë ¤ì£¼ì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤. ì™œëƒë©´:
1. We didnâ€™t tell theÂ `Trainer`Â to evaluate during training by settingÂ `evaluation_strategy`Â to eitherÂ `"steps"`Â (evaluate everyÂ `eval_steps`) orÂ `"epoch"`Â (evaluate at the end of each epoch).
2. We didnâ€™t provide theÂ `Trainer`Â with aÂ `compute_metrics()`Â function to calculate a metric during said evaluation (otherwise the evaluation would just have printed the loss, which is not a very intuitive number).

### Evaluation
Letâ€™s see how we can build a usefulÂ `compute_metrics()`Â function and use it the next time we train. The function must take anÂ `EvalPrediction`Â object (which is a named tuple with aÂ `predictions`Â field and aÂ `label_ids`Â field) and will return a dictionary mapping strings to floats (the strings being the names of the metrics returned, and the floats their values). To get some predictions from our model, we can use theÂ `Trainer.predict()`Â command:

```python
predictions = trainer.predict(tokenized_datasets["validation"])
print(predictions.predictions.shape, predictions.label_ids.shape)
```

```python
(408, 2) (408,)
```

The output of theÂ `predict()`Â method is another named tuple with three fields:Â `predictions`,Â `label_ids`, andÂ `metrics`. TheÂ `metrics`Â field will just contain the loss on the dataset passed, as well as some time metrics (how long it took to predict, in total and on average). Once we complete ourÂ `compute_metrics()`Â function and pass it to theÂ `Trainer`, that field will also contain the metrics returned byÂ `compute_metrics()`.

ë³´ì‹œë‹¤ì‹œí”¼ `predictions` ëŠ”Â 408 x 2 ëª¨ì–‘ì˜ 2ì°¨ì› ë°°ì—´ì…ë‹ˆë‹¤(408ì€ ìš°ë¦¬ê°€ ì‚¬ìš©í•œ ë°ì´í„°ì„¸íŠ¸ì˜ ìš”ì†Œ ìˆ˜ì…ë‹ˆë‹¤). ì´ëŠ” ìš°ë¦¬ê°€ ì „ë‹¬í•œ ë°ì´í„° ì„¸íŠ¸ì˜ ê° ìš”ì†Œì— ëŒ€í•œ ë¡œì§“ì…ë‹ˆë‹¤ (Â [ì´ì „ ì¥](https://huggingface.co/course/chapter2)`predict()`Â ì—ì„œ ë³¸ ê²ƒì²˜ëŸ¼Â ëª¨ë“  Transformer ëª¨ë¸ì€ ë¡œì§“ì„ ë°˜í™˜í•©ë‹ˆë‹¤). ì´ë¥¼ ë ˆì´ë¸”ê³¼ ë¹„êµí•  ìˆ˜ ìˆëŠ” ì˜ˆì¸¡ìœ¼ë¡œ ë³€í™˜í•˜ë ¤ë©´ ë‘ ë²ˆì§¸ ì¶•ì—ì„œ ìµœëŒ€ê°’ì„ ê°–ëŠ” ì¸ë±ìŠ¤ë¥¼ ê°€ì ¸ì™€ì•¼ í•©ë‹ˆë‹¤.

```python
import numpy as np

preds = np.argmax(predictions.predictions, axis=-1)
```

ì´ì œ ì´ë¥¼Â `preds`ë¼ë²¨ê³¼ ë¹„êµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•¨ìˆ˜ë¥¼Â êµ¬ì¶•í•˜ê¸° ìœ„í•´ ğŸ¤—Â [í‰ê°€](https://github.com/huggingface/evaluate/)Â ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ `compute_metric()` function ì„ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤Â . ì´ë²ˆì—ëŠ” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì„¸íŠ¸ë¥¼ ë¡œë“œí•œ ê²ƒì²˜ëŸ¼ ì‰½ê²Œ MRPC ë°ì´í„°ì„¸íŠ¸ì™€ ê´€ë ¨ëœ `evaluate.load()` í•¨ìˆ˜ë¥¼ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤Â . ë°˜í™˜ëœ ê°ì²´ì—ëŠ”Â ì¸¡ì •í•­ëª© ê³„ì‚°ì„ ìˆ˜í–‰í•˜ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” `compute()` ë©”ì„œë“œê°€ ìˆìŠµë‹ˆë‹¤.

```python
import evaluate

metric = evaluate.load("glue", "mrpc")
metric.compute(predictions=preds, references=predictions.label_ids)
```

```python
{'accuracy': 0.8578431372549019, 'f1': 0.8996539792387542}
```

ëª¨ë¸ í—¤ë“œì˜ ë¬´ì‘ìœ„ ì´ˆê¸°í™”ë¡œ ì¸í•´ ì–»ì€ ì¸¡ì •í•­ëª©ì´ ë³€ê²½ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì–»ì„ ìˆ˜ ìˆëŠ” ì •í™•í•œ ê²°ê³¼ëŠ” ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ ìš°ë¦¬ ëª¨ë¸ì˜ ê²€ì¦ ì„¸íŠ¸ ì •í™•ë„ëŠ” 85.78%ì´ê³  F1 score ëŠ” 89.97ì„ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” GLUE ë²¤ì¹˜ë§ˆí¬ì— ëŒ€í•œ MRPC ë°ì´í„°ì„¸íŠ¸ì˜ ê²°ê³¼ë¥¼ í‰ê°€í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ë‘ ê°€ì§€ ì¸¡ì •í•­ëª©ì…ë‹ˆë‹¤.Â [BERT ë…¼ë¬¸](https://arxiv.org/pdf/1810.04805.pdf)Â ì˜ í‘œëŠ”Â ê¸°ë³¸ ëª¨ë¸ì˜ F1 ì ìˆ˜ê°€ 88.9ë¼ê³  ë³´ê³ í–ˆìŠµë‹ˆë‹¤. ê·¸ê²ƒì€Â ìš°ë¦¬ê°€ í˜„ì¬ `uncased`Â ëª¨ë¸ì„ ì‚¬ìš©í•˜ê³  ìˆëŠ” ë™ì•ˆì˜ `cased` ëª¨ë¸ì´ì—ˆëŠ”ë°, ì´ê²ƒì´ ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ì„¤ëª…í•©ë‹ˆë‹¤.

ëª¨ë“  ê²ƒì„ í•˜ë‚˜ë¡œ ë¬¶ìœ¼ë©´ ë‹¤ìŒê³¼ ê°™ì€Â `compute_metrics()` function ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
def compute_metrics(eval_preds):
    metric = evaluate.load("glue", "mrpc")
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)
```

ê·¸ë¦¬ê³  ê° epochê°€ ëë‚  ë•Œ metricì„ reportí•˜ê¸° ìœ„í•´ ì‹¤ì œë¡œ ì‚¬ìš©ë˜ëŠ” ê²ƒì„ í™•ì¸í•˜ê¸° ìœ„í•´Â `Trainer`ì´Â `compute_metrics()`í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒˆ í•­ëª©ì„ ì •ì˜í•˜ëŠ” ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

```python
training_args = TrainingArguments("test-trainer", evaluation_strategy="epoch")
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
```

Note that we create a newÂ `TrainingArguments`Â with itsÂ `evaluation_strategy`Â set toÂ `"epoch"`Â and a new model â€” otherwise, we would just be continuing the training of the model we have already trained. To launch a new training run, we execute:

```python
trainer.train()
```

ì´ë²ˆì—ëŠ” í›ˆë ¨ ì†ì‹¤ ì™¸ì— ê° ì—í¬í¬ê°€ ëë‚  ë•Œë§ˆë‹¤ ê²€ì¦ ì†ì‹¤ê³¼ ì¸¡ì •í•­ëª©ì„ ë³´ê³ í•©ë‹ˆë‹¤. ë‹¤ì‹œ ë§í•˜ì§€ë§Œ, ë„ë‹¬í•œ ì •í™•í•œ ì •í™•ë„/F1 ì ìˆ˜ëŠ” ëª¨ë¸ì˜ ë¬´ì‘ìœ„ í—¤ë“œ ì´ˆê¸°í™”ë¡œ ì¸í•´ ìš°ë¦¬ê°€ ì°¾ì€ ê²ƒê³¼ ì•½ê°„ ë‹¤ë¥¼ ìˆ˜ ìˆì§€ë§Œ ë™ì¼í•œ ê¸°ì¤€ì ì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤.

ì´ëŠ”Â `Trainer`ì—¬ëŸ¬ GPU ë˜ëŠ” TPUì—ì„œ ì¦‰ì‹œ ì‘ë™í•˜ë©° í˜¼í•© ì •ë°€ë„ êµìœ¡(Â `fp16 = True`êµìœ¡ ì¸ìˆ˜ì— ì‚¬ìš©)ê³¼ ê°™ì€ ë‹¤ì–‘í•œ ì˜µì…˜ì„ ì œê³µí•©ë‹ˆë‹¤. 10ì¥ì—ì„œ ì§€ì›ë˜ëŠ” ëª¨ë“  ë‚´ìš©ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

ì´ê²ƒìœ¼ë¡œ APIë¥¼ ì‚¬ìš©í•œ ë¯¸ì„¸ ì¡°ì •ì— ëŒ€í•œ ì†Œê°œë¥¼ ë§ˆì¹©ë‹ˆë‹¤Â `Trainer`. ê°€ì¥ ì¼ë°˜ì ì¸ NLP ì‘ì—…ì— ëŒ€í•´ ì´ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ì˜ˆëŠ”Â [7ì¥](https://huggingface.co/course/chapter7)Â ì—ì„œ ì œê³µë˜ì§€ë§ŒÂ ì§€ê¸ˆì€ ìˆœìˆ˜í•œ PyTorchì—ì„œ ë™ì¼í•œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.




# ì§€ê¸ˆê¹Œì§€ëŠ” í•¨ìˆ˜ë“¤ì´ ìë™ìœ¼ë¡œ í•´ì¤¬ë˜ ë¶€ë¶„ë“¤ì´ ë§ì•„ì„œ ì²˜ìŒë¶€í„° ì–´ë–»ê²Œ fine-tuning í•˜ëŠ”ì§€ ë³´ê³ ì‹¶ìœ¼ë©´ [ì—¬ê¸°ë¡œ](https://huggingface.co/learn/nlp-course/chapter3/4?fw=pt)