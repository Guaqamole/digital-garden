---
title: 신경망 손실함수
date: 2024-05-02
draft: false
tags:
  - DeepLearning
complete: true
---
사람들에게 "지금 얼마나 행복하나요?" 라고 물으면 뭐라고 대답할까요? 
"아주행복하죠"나"그 리행복한거같진않아요"라는막연한답이돌아오는게보통입니다. 

그런 가운데 누군가 "현재 내 행복 지수는 10.23 입니다" 라고 대답한다면 질문 한 사람이 당황해버리겠죠.

이 ' 행 복 지 표' 이 야 기 는 하 나 의 비 유 지 만, 실 은 신 경 망 학 습 에 서 도 이 와 같 은 일 을 수 행 합 니 다. 

신경망 학습에서는 현재의 상태를 '하나의지표' 로 표현 합니다. 
그리고 그 지표를 가장좋게 만들어주는 가중치 매개 변수의 값을 탐색하는것입니다. 

'행복지표'를 가진 사람이 그지표를 근거로 '최적의인생' 을 탐색하듯,
<mark style="background: #C6AB16;">신경망도 '하나의 지표'를 기준으로 최적의 매개변수 값 을 탐색하는데 그 지표의 이름은 바로 손실 함수 (loss function) 라고 합니다.</mark>

손실 함수는 임의의 함수를 사용할 수도 있지만 일반적으로는, **1) 오차제곱합  2) 교차 엔트로피 오차**를 사용합니다.

>[!warning] 
> 손실 함수는 신경망 성능의 '나쁨'을 나타내는 지표로, 현재의 신경망이 훈련 데이터를 얼마나 잘 처리하지 못하느냐를 나타냅니다. 성능 나쁨을 지표로한다니 무언가 부자연스럽다고 생각할지모르지만, 손실 함수에 마이너스만 곱하면 ‘얼마나 나쁘지않나' ,즉, '얼마나좋으냐' 라는지 표로 변신하죠.또 ‘나쁨을 최소로 하는것' 과 '좋음을 최대로 하는것’은 결국 같은것이니까. 성능의' 나쁨' 과 ‘좋음' 중 어느쪽을지 표로삼아도 본질적으로수행하는 일은 다르지 않습니다.

## SSE (Sum of Squares for error)
신경망에서 가장 많이 쓰이는 오차제곱합의 수식은 다음과 같습니다.
$$ E = \frac{1}{2}\sum^{}_{k}(y_{k} - t_{k})^2 $$
- yk 는 신경망의 출력 (신경망이 추정한 값)
- tk 는 정답 레이블
- k 는 데이터의 차원 수


예시) 원소 10개 짜리 손글씨 숫자 
![|525](https://i.imgur.com/dB9yuid.png)

이배열들의원소는첫번째인덱스부터순서대로숫자'0,'1','2,. .일때의값입니다

```python
>> y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0] 
>> # 0, 1, 2, 3, 4, 5 ...
>> t = [0,0,1,0,0,0,0,0,0,0]
```

여기에서 신경망의 출력y는 소프트맥스함수의출력입니다.
이미지가' 0일확률은0.1,''일확률은0.05 ‘2'일 확률은 0.6이 라 고 해 석 되 죠. 

한 편 정 답 레 이 블 인 t 는 정 답 을 가 리 키 는 위 치 의 원 소 는 1 로 , 그 외 에 는 0 으 로 표기합니다. 
여기에서는숫자'2'에해당하는원소의값이1이므로정답이'2'임을알수있습니다.

→ 이처럼 한원소만1로하고그외는0으로나타내는표기법을 원-핫 인코딩 (one-hot encoding) 이라한다고했습 니다.

### 구현
```python
def sum_squares_error(y, t): 
    return 0.5 * np.sum((y-t)**2)
```

### 추론
```python
# 정답은 2
t = [0,0,1,0,0,0,0,0,0,0]

# 예: 2일 확률이 가장 높다고 추정함 (0.6)
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0] 
sum_squares_error(np.array(y),np.array(t)) # 0.0975

# 예: 7일 확률이 가장 높다고 추정함 (0.6)
y= [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]
sum_squares_error(np.array(y),np.array(t)) # 0.5975
```
첫번째예의손실함수쪽출력이작으며정답레이블과의오차도작은 것을알수있습니다.

즉, 오차제곱합기준으로는첫번째추정결과가(오차가더작으니) 정 답에더가까울것으로판단할수있습니다.

## CEE (Cross Entropy Error)
교차엔트로피 오차도 자주 이용합니다. 교차엔트로피오 차의수식은다음과같습니다:
$$ E = -\sum_{k} t_{k} \log y_{k}$$
- log 밑이 e 인 자연로그 (log e)
- yk 는 신경망의 출력 
- tk 는 정답 레이블이며 원핫 인코딩

예시) 정답 레이블은 2가 정답이라고 하고 신경망 출력이 0.6 이라면 교차 엔트로피 오차는 -log * 0.6 = 0.51 이 됩니다.
즉, 교차 엔트로피 오차는 정답일때의 출력이 전체값을 정하게 됩니다.
![|650](https://i.imgur.com/A9VxSGR.png)

### 구현
```python
def cross_entropy_error(y, t):
	delta = 1e-7
	return -np.sum(t * np.log(y + delta))
```

np.log를계산할때아주작
은값인delta를더했습니다. 이는np.log() 함수에0을입력하면마이너스무한대를뜻하는 - inf가되어더이상계산을진행할수없게되기때문입니다. 아주작은값을더해서절대0이 되지않도록,즉마이너스무한대가발생하지않도록한것이죠.


## 만약 데이터 사이즈가 크다면? 미니배치학습.
지금까지데이터하나에대한손실함수만생각해왔으니, 이제훈련데이터모두에대한손실 함수의합을구하는방법을생각해보겠습니다.

$$ E = -\frac{1}{N} \sum_{n} \sum_{k} t_{nk} \log y_{nk} $$
이때데이터가N개라면있는1번째데이터의k번째값을의미합니다(V 는신경망의출력, 는정답레이블입니다). 수식이좀복잡해보이지만데이터하나에대한손실함수인 위식을 단순히N개의데이터로확장했을뿐입니다.

그런데MNIST데이터셋은훈련데이터가60,000개였습니다. 그래서모든데이터를대상으로
손실함수의합을구하려면시간이좀걸립니다.

이 런 경 우 데 이 터 일 부 를 추 려 전 체 의 ' 근 사 치' 로 이 용 할수있습니다. 신경망학습에서도훈련데이터로부터일부만골라학습을수행합니다. 이일부 를 미니배치 (mini-batch) 라고하죠.

```python
import sys, os
sys.path.append(os.pardir)

from dlscratch.dataset.mnist import load_mnist

(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)
# 각 데이터의 형상 출력 
print(x_train.shape) # (60000, 784) 
print(t_train.shape) # (60000, 10)
```

이훈련데이터에서무작위로10장만빼내려면어떻게하면될까요? 넘파이의 np. random. choice () 함수를쓰면다음과같이간단히해결할수있습니다.

```python
train_size = x_train.shape[0]
batch_size = 10
batch_mask = np.random.choice(train_size, batch_size)
x_batch = x_train[batch_mask]
t_batch = t_train[batch_mask]
```


미니 배치로 교차 엔트로피 구하기.
```python
def cross_entropy_error(y, t): 
	if y.ndim == 1:
		t = t.reshape(1, t.size) 
		y = y.reshape(1, y.size)

	batch_size = y.shape[0]
	return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size
```



## 왜 정확도 대신 손실 함수를?
> [!info] 스포주의
>  신경망을 학습할때 정확도를 지표로 삼아서는안된다. 정확도를 지표로 하면매개변수의 미분이 대부분의 장소에서 0이되기때문이다.
>  → 정확도는매개변수의미소한변화에는거의반응을보이지않고, 반응이있더라도그값이 불연속적으로갑자기변화합니다.이는계' 단함수를활성화함수로사용하지않는이유와도들어맞습니다.


'정확도'라는지표를놔두고'손실함수의값'이라는우회적인 방법을택하는이유는뭘까요?

이의문은신경망학습에서의'미분'의역할에주목한다면해결됩니다. 신경망 학습에서는 최적의 매개변수(가중치와편향)를 탐색 할때 손실함수 의값을가능한한작게하는매개변수값을찾습니다.

이때, 매개변수의미분(정확히는기울기)을 계산하고, 그미분값을 단서로 매개변수의 값을 서서히 갱신하는 과정을 반복합니다.

가령 여기에 가상의신경망이있고 그신경망의어느한가중치매개변수에주목한다고합시다.

**가중치 매개변수의 손실 함수의 미분: '가중치매개변수의값을아주조금변화 시켰을때, 손실함수가어떻게변하나'라는의미**

만약 이 미분값이 음수면그가중치 매개변수를양의방향으로변화시켜손실함수의값을줄일수있습니다. 
반대로, 미분값이 양수면 가중치매개변수를 음의 방향으로 변화시켜 손실함수의값을줄일수있습니다.

<mark style="background: #C6AB16;">그러나 미분값이0이면가중치매개변수를어느쪽으로움직여도손실함수의값은줄어들지 않습니다. 그래서가중치매개변수의갱신은거기서멈춥니다.</mark>

→ 정확 도를 지 표로 삼아서 는 안 되는 이 유는 미 분 값이 대 부분의 장소에서 0 이 되어 매 개 변수를 갱신할수없기때문입니다.


예시) 한신경망이100장의훈련데이터중32장을올바로인식한다고 합니다
그렇다면정확도는32%입니다. 만약 정확도가 지표였다면 가중치매개변수의값을조금바꾼다고해도정확도는그대로32%일겁니다. 
즉, 매개변수를약간만조정해서는 정확도가 개선되지않고 일정하게유지됩니다. 혹, 정확도가개선된다하더라도 
그값은32.0123%와 같은연속적인변화보다는 33%나 34%처럼 불연속적인 띄엄띄엄한 값으로 바뀌어버립니다.

한편,손실함수를지표로삼았다면어떨까요?현재의손실함수의값은0.92543. 같은 수치로나타납니다. 그리고매개변수의값이조금변하면그에반응하여손실함수의값도 0.93432. .처럼연속적으로변화하는것입니다.

#### 계단 함수를 사용하지 않는 이유
계단함수의미분은 그림4- 4 와같이대부분의장소(0 이외의곳)에서0입니다. 그결과, 계단함수를이용하면손실함수를지표로삼는게아무의 미가없게됩니다. 매개변수의작은변화가주는파장을계단함수가말살하여손실함수의값 에는아무런변화가나타나지않기때문입니다.
![](https://i.imgur.com/fswBSFf.png)

<mark style="background: #C6AB16;">계단함수는한순간만변화를일으키지만, 
시그모이드함수의미분(접선)은 그림4-4 와같 이출력(세로축의값)이연속적으로변하고곡선의기울기도연속적으로변합니다. 
즉, 시그 모이드함수의미분은어느장소라도0 이되지는않습니다. </mark>
이는신경망학습에서중요한성질 로, 기울기가0이되지않는덕분에신경망이올바르게학습할수있는것입니다.
