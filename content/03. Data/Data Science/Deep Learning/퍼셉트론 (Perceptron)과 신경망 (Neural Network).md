---
title: 퍼셉트론 (Perceptron)
date: 2024-04-30
draft: false
tags:
  - DeepLearning
  - Concept
complete: true
---
### Definition
'인공뉴런' :  ' 가 중 치' 와 ' 편 향' 을 매 개 변 수 로 설정한 입출력을 갖춘 알고리즘이다!
퍼셉트론은다수의신호를입력으로받아하나의신호를출력합니다. 여기서말하는신호란 전류나강물처럼흐름이있는것을상상하면좋습니다. 전류가전선을타고흐르는전자를내보 내듯, 퍼셉트론신호도흐름을만들고정보를앞으로전달합니다.

다만, 실제 전류와달리 퍼셉 트론신호는 '흐른다/안흐른다(1이나0)'의 두가지값을가질수있습니다. 이책에서는1을 ' 신호가흐른다' ,0을' 신호가흐르지않는다' 라는의미로쓰겠습니다.

### Weight and Bias
[[Logical Gates]] 참고

### 퍼셉트론의 한계
### XOR 게이트 → 배타적 논리합
지금까지본퍼셉트론으로는이XOR게이트를구현할수없습니다. 왜AND와OR는 되고XOR는안될까요? 그림으로그려가며시각적으로설명해보겠습니다.
우선OR게이트의동작을시각적으로생각해보죠. OR게이트는, 예를들어가중치매개변수 가( b,w, ,v.)=( -0.5,1.0.1.0)일때 그림2 -4의진리표를만족합니다. 
이때의퍼셉트론 은 식 2 . 3 으 로 표 현 됩 니 다.


### Step Function
```python
import numpy as np
import matplotlib.pylab as plt


def step_function(x):
    y = x > 0
    return y.astype(np.int64)

x = np.arange(-5.0, 5.0, 0.1)
y = step_function(x)
plt.plot(x, y)
plt.ylim(-0.1, 1.1)
plt.show()
```

![|625](https://i.imgur.com/5OzUsan.png)

### Sigmoid Function
```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

x = np.arange(-5.0, 5.0, 0.1)
y = sigmoid(x)
plt.plot(x, y)
plt.ylim(-0.1, 1.1)
plt.show()
```

![|625](https://i.imgur.com/BfyYEDX.png)


### Sigmoid vs Step Function
가장먼저느껴지는점은'매끄러움'의차이일것입니다. 

시그모이드함수 는부드러운곡선이며입력에따라출력이연속적으로변화합니다. 
한편, 계단함수는0을경 계로출력이갑자기바뀌어버립니다. 시그모이드함수의이매끈함이신경망학습에서아주중 요한역할을하게됩니다.

(역시매끈함과관련되지만) 계단함수가0과1 중하나의값만돌려주는 반면
시그모이드함수는실수(0. 731..., 0.880.. 등) 를돌려준다는점도다릅니다. 

다시말해 퍼셉트론에서는 뉴런 사이에0 혹은1이흘렀다면, 신경망에서는 연속적인실수가흐릅니다.
비유하자면, 계단함수는'시시오도시'*이고시그모이드함수는'물레방아'와비슷하죠. 
계단함수는시시오도시처럼물을쏟아내거나쏟아내지않는(0 또는1) 두가지움직임을보여주며, 
시그모이드함수는물레방아처럼흘러온물의양에비례해흐르는물의양을조절합니다.

#### 비선형 함수 (non-linear)
신경망에서는활성화함수로비선형함수를사용해야합니다. 
달리말하면선형함수를사용해서는안됩니다. 

**왜선형함수는안되는걸까요?** 
**→ 그이유는 바로 선형함수를이용하면 신경망의 층을 깊게하는 의미가 없어지기 때문입니다.**

### Perceptron vs Neural Network
#### 퍼셉트론의 한계
- 퍼셉트론으로 복잡한 함수도 표현할 수 있다.
- 컴퓨터가 수행하는 복잡한 처리도 퍼셉트론으로 표현할수 있다.
- 하지만 가중치를 설정하는 작업은 여전히 사람이 수동으로 해야한다.
- AND OR 게이트 표를 보면서 인간이 적절한 ‘가중치’ 값을 조절해야했었다.
#### 신경망으로 
- 신경망은 가중치 매개변수의 적절한 값을 데이터로 부터 자동으로 학습하는 성질을 가지고있다.
- 그래서 우리는 신경망이 입력 데이터를 식별하고 어떻게 처리하는지 과정을 아는것이 중요하다.
![|750](https://i.imgur.com/g8QVaDr.png)


### 출력층
신경망은분류와회귀모두에이용할수있습니다. 다만둘중 어떤문제냐에 따라출력층에서 사용하는 활성화 함수가 달라집니다. 
일반적으로 **회귀에는 항등함수**
**분류에는 소프트맥스 함수를사용합니다**

#### 항등 함수 (identity function)
항등함수idenitlyhunction는입력을그대로출력합니다. 입력과
출력이항상같다는뜻의항등입니다
→ 출력층에서 항등 함수를 사용하면 입력 신호가 그대로 출력 신호가 된다.

#### 소프트맥스 함수 (softmax)
한편, 분류에서사용하는소프트맥스함수 soft max function의식은다음과같습니다.

$$ Y_{k} = \frac{exp(a_{k})}{\sum_{i=1}^nexp(a_{i})} $$
