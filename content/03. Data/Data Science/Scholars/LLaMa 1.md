---
title: LLaMa 1
date: 2024-08-13
draft: false
tags:
  - Scholars
  - LLM
complete: true
link: https://arxiv.org/pdf/2302.13971v1
---
## Abstract
- 7B ~ 65B 개 파라미터
- 우리는 이 모델들을 수조 개의 토큰에 대해 훈련시키며, 공개적으로 이용 가능한 데이터셋만을 사용하여도 최첨단 모델을 훈련시킬 수 있다는 것을 보여줍니다. 이러한 방식은 독점적이고 접근하기 어려운 데이터셋에 의존하지 않습니다.
- 모든 연구모델을 연구 커뮤니티에 공개

## Introduction
> [!summary] 연구의 중점
> 일반적으로 사용되는 것보다 더 많은 토큰에 대해 훈련하여 다양한 추론 예산에서 최상의 성능을 달성하는 일련의 언어 모델을 훈련하는 것. 결과적으로 나온 모델들은 LLaMA라고 부르며, 이는 70억(7B)에서 650억(65B) 까지의 파라미터를 가지고 있고, 기존의 최고의 LLMs와 경쟁력 있는 성능을 보임
> 예) LLaMA-13B는 크기가 GPT-3보다 10배 작음에도 불구하고 대부분의 벤치마크에서 GPT-3를 능가
> 


- 최근엔 모델을 충분한 크기로확장할때 학습 능력 증가가 처음으로 나타났으며 (Kaplan 등, 2020), 이로 인해 이러한 모델을 더욱 확장하는 연구들이 진행
- 더많은파라미터가더나은성능을이끌어낼것이라는 가정에 기반
- 그러나 최근 Hoffmann 등(2022)의 연구에서는 주어진 컴퓨팅 예산으로 **최상의 성능을 달성 하는것은 가장큰 모델이아니라더많은데이터로 훈련된 더 작은 모델임을 보여주고 있다.**
- 특정 성능 수준에 도달하기 위해 큰모델을 훈련 하는것이 더 저렴할수 있지만, **더 오래 훈련된 작은 모델은 결국 추론에서 더 저렴할 것입니다.**
	- 예) Hoffmann 등(2022)은 2000억 토큰에 대해 100억 크기의 모델을 훈련할 것을 권장하지만, 우리는 70억 크기의 모델의 성능이 1조 토큰 이후에도 계속 향상된다는 것을 발견

## Approach
- 훈련 접근법은 이전 연구(Brown 등, 2020; Chowdhery 등, 2022) 에서 설명된 방법과 유사하며, Chinchilla 스케일링 법칙(Hoffmann 등, 2022)에 영감을 받음
- **Meta는 표준 Optimizer를 사용하여 대량의 텍스트 데이터에 대한 큰 트랜스포머를 훈련시킴**

### pre-training data
- 전반적으로, 토큰화 후에 우리의 전체 훈련 데이터셋은 대략 1.4T(1.4 조개)의 토큰을 포함
- 대부분의 훈련 데이터에서 각 토큰은 훈련중 한번만 사용
- 단, 위키피디아와 Books 도메인은 예외로, 이들에 대해서는 대략 두 번의 에포크(epoch)를 수행

### architecture
최근 대규모 언어 모델에 대한 연구를 따라, Meta의 네트워크는 트랜스포머 아키텍처(Vaswani et al., 2017)를 기반으로 하고 있습니다.
• Meta는 이후에 제안된 다양한 개선사항을 활용하고, PaLM과 같은 다른 모델에서 사용된 것을 참고함

### optimizer
Meta의 모델은 AdamW 옵티마이저 (Loshchilov and Hutter, 2017)를 사용하여 훈련되며, 다음과 같은 하이퍼파라미터를 사용합니다: β1=0.9, β2=0.95
- 코사인 학습률 일정을 사용하여, 최종 학습률이 최대 학습률의 10%가 되도록 함
- 또한 가중치 감쇠로 0.1을, 그래디언트 클리핑으로 1.0을 사용
- 우리는 2,000번의 웜업 스텝을 사용하며, 모델의 크기에 따라 학습률과 배치 크기를 달리함

### efficient implementaion
- 65B(650억) 파라미터 모델을 훈련할 때, 우리의 코드는 80GB의 RAM을 가진 2048개의 A100 GPU에서 초당 약 380 토큰을 처리
- 이는 우리의 데이터셋에 1.4T(1.4조개) 토큰이 포함되어 있을 경우, 훈련이 대략 21일이 걸린다는 것을 의미

## Main Results
우리는 LLaMA와 다른 기반 모델, 즉 공개되지 않은 언어 모델인 GPT-3 (Brown et al., 2020), Gopher (Rae et al., 2021), Chinchilla (Hoffmann et al., 2022), 그리고 PaLM (Chowdhery et al., 2022)을 비교합니다.

우리는 LLaMA를 자유 형식 생성(free-form generation) 작업과 객관식 (multiple choice) 작업에서 평가합니다.

객관식 작업에서의 목표는 주어진 문맥에 기반하여 주어진 옵션 중에서 가장 적절한 완성을 선택하는 것입니다.


## Conclusion
- 이 논문에서는 공개적으로 릴리스되며 최첨단 기반 모델과 경쟁력 있는 일련의 언어 모델을 소개했습니다. 가장 주목할 만한 것은 LLaMA-13B가 GPT-3를 능가하면서도 10배 이상 작다는 것이며, LLaMA-65B는 Chinchilla-70B와 PaLM-540B와 경쟁력을 가집니다.
    
- 이전 연구와 달리, 독점 데이터셋을 사용하지 않고도 공개적으로 이용 가능한 데이터만을 사용하여 최첨단 성능을 달성할 수 있다는 것을 보여줍니다.
    
- 이러한 모델을 연구 커뮤니티에 공개함으로써 대규모 언어 모델의 발전을 가속화하고, 그들의 견고성을 향상시키고 악성 및 편향과 같은 알려진 문제를 완화하는 노력에 도움이 될 것이라고 기대합니다.
    
- 추가로, Chung 등(2022)과 같이 이러한 모델을 지침에 대한 미세 조정(finetuning)이 유망한 결과를 가져온다는 것을 확인했으며, 이를 미래의 연구에서 더 깊게 조사할 계획입니다.
    
- 마지막으로, 우리는 규모를 키우면서 지속적인 성능 향상을 보았기 때문에, 미래에는 더 큰 모델을 더 큰 사전 훈련 데이터셋에서 훈련시켜 릴리스할 계획입니다.